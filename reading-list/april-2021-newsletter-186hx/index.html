<!DOCTYPE html><html lang="zh-CN" data-astro-cid-sckkx6r4> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v5.16.1"><meta name="description" content="with links on AI scaling, particular new East Asian record-breaking work &#38; deep reinforcement learning."><link rel="canonical" href="https://blog.yuyins.com/reading-list/april-2021-newsletter-186hx/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://blog.yuyins.com/reading-list/april-2021-newsletter-186hx/"><meta property="og:title" content="April 2021 newsletter"><meta property="og:description" content="with links on AI scaling, particular new East Asian record-breaking work &#38; deep reinforcement learning."><meta property="og:image" content="https://blog.yuyins.com/favicon.svg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://blog.yuyins.com/reading-list/april-2021-newsletter-186hx/"><meta property="twitter:title" content="April 2021 newsletter"><meta property="twitter:description" content="with links on AI scaling, particular new East Asian record-breaking work &#38; deep reinforcement learning."><meta property="twitter:image" content="https://blog.yuyins.com/favicon.svg"><title>April 2021 newsletter</title><!-- Theme and color scheme script (runs before page loads to prevent flash) --><script>
			// 初始化配色方案 - 默认使用 stone-amber
			const colorScheme = localStorage.getItem('colorScheme') || 'stone-amber';
			document.documentElement.setAttribute('data-color', colorScheme);

			// 强制使用浅色主题
			document.documentElement.setAttribute('data-theme', 'light');
		</script><link rel="stylesheet" href="/_astro/_slug_.C-G01a-E.css">
<style>.blog-post-container[data-astro-cid-wrgicudb]{width:100%;margin:0 auto;padding:2rem 0}.blog-post[data-astro-cid-wrgicudb]{width:100%;min-width:0;max-width:42rem;margin:0 auto}.article-header[data-astro-cid-wrgicudb]{margin-bottom:2rem;padding-bottom:1rem;border-bottom:1px solid var(--border)}.article-header[data-astro-cid-wrgicudb] h1[data-astro-cid-wrgicudb]{font-size:2rem;line-height:1.3;margin-bottom:1rem;font-weight:700;color:var(--foreground);word-break:break-word}.article-meta[data-astro-cid-wrgicudb]{display:flex;align-items:center;gap:.5rem;font-size:.875rem;color:var(--muted-foreground)}.source-tag[data-astro-cid-wrgicudb]{background-color:var(--muted);color:var(--foreground);padding:.125rem .5rem;border-radius:9999px;font-weight:500;font-size:.75rem;text-decoration:none;transition:opacity .2s}.source-tag[data-astro-cid-wrgicudb]:hover{opacity:.8}.separator[data-astro-cid-wrgicudb]{color:var(--border)}.article-content[data-astro-cid-wrgicudb]{line-height:1.8;margin-bottom:3rem;font-size:1.0625rem;color:var(--foreground)}.article-content[data-astro-cid-wrgicudb] h2{margin-top:2rem;margin-bottom:1rem;font-size:1.5rem;font-weight:600}.article-content[data-astro-cid-wrgicudb] h3{margin-top:1.5rem;margin-bottom:.75rem;font-size:1.25rem;font-weight:600}.article-content[data-astro-cid-wrgicudb] p{margin-bottom:1.25rem}.article-content[data-astro-cid-wrgicudb] a{color:var(--accent);text-decoration:underline;text-underline-offset:2px}.article-content[data-astro-cid-wrgicudb] img{max-width:100%;height:auto;border-radius:.5rem;margin:1.5rem 0}.article-content[data-astro-cid-wrgicudb] blockquote{border-left:4px solid var(--border);padding-left:1rem;margin:1.5rem 0;color:var(--muted-foreground);font-style:italic}.article-content[data-astro-cid-wrgicudb] pre{background:var(--muted);padding:1rem;border-radius:.5rem;overflow-x:auto;font-family:monospace;margin:1.5rem 0}.article-content[data-astro-cid-wrgicudb] ul,.article-content[data-astro-cid-wrgicudb] ol{padding-left:1.5rem;margin-bottom:1.25rem}.article-content[data-astro-cid-wrgicudb] li{margin-bottom:.5rem}.article-footer[data-astro-cid-wrgicudb]{padding-top:2rem;border-top:1px solid var(--border);display:flex;flex-direction:column;gap:1.5rem;align-items:center}.action-row[data-astro-cid-wrgicudb]{display:flex;justify-content:center}.read-original-btn[data-astro-cid-wrgicudb]{display:inline-flex;align-items:center;gap:.5rem;padding:.75rem 1.5rem;background-color:var(--foreground);color:var(--background);border-radius:.5rem;font-weight:500;text-decoration:none;transition:opacity .2s}.read-original-btn[data-astro-cid-wrgicudb]:hover{opacity:.9}.back-to-list[data-astro-cid-wrgicudb]{text-align:center}.back-link[data-astro-cid-wrgicudb]{color:var(--muted-foreground);text-decoration:none;font-size:.875rem;transition:color .2s}.back-link[data-astro-cid-wrgicudb]:hover{color:var(--foreground)}
</style></head> <body data-astro-cid-sckkx6r4> <a id="skip-to-content" href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:top-4 focus:left-4 focus:z-50 focus:px-4 focus:py-2 focus:bg-accent focus:text-background" data-astro-cid-sckkx6r4>Skip to content</a> <header class="header" data-astro-cid-3ef6ksr2> <nav class="header-nav" aria-label="Main navigation" data-astro-cid-3ef6ksr2> <a href="/" class="header-logo" data-astro-cid-3ef6ksr2>
AstroBlog
</a> <div class="header-links" data-astro-cid-3ef6ksr2> <ul class="nav-list" data-astro-cid-3ef6ksr2> <li data-astro-cid-3ef6ksr2> <a href="/" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 文章 </a> </li><li data-astro-cid-3ef6ksr2> <a href="/category" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 分类 </a> </li><li data-astro-cid-3ef6ksr2> <a href="/reading-list" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 订阅 </a> </li><li data-astro-cid-3ef6ksr2> <a href="/about" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 关于 </a> </li> </ul> <div class="header-actions" data-astro-cid-3ef6ksr2> <div id="search" data-astro-cid-otpdt6jm> <button id="search-button" class="search-toggle" aria-label="打开搜索" title="搜索 (⌘K)" data-astro-cid-otpdt6jm> <svg class="search-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-astro-cid-otpdt6jm> <circle cx="11" cy="11" r="7" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-otpdt6jm></circle> <path d="M20 20L16.5 16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-otpdt6jm></path> </svg> </button> <dialog id="search-dialog" class="search-modal" data-astro-cid-otpdt6jm> <div class="modal-content" data-astro-cid-otpdt6jm> <div class="modal-header" data-astro-cid-otpdt6jm> <h2 class="modal-title" data-astro-cid-otpdt6jm>搜索文章</h2> <button id="close-search" class="close-button" aria-label="关闭搜索" data-astro-cid-otpdt6jm> <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-astro-cid-otpdt6jm> <path d="M18 6L6 18M6 6l12 12" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" data-astro-cid-otpdt6jm></path> </svg> </button> </div> <div id="pagefind-ui" data-astro-cid-otpdt6jm></div> </div> </dialog> </div> <script type="module">let d=!1;const c="pagefind-loaded";async function l(t=0){if(sessionStorage.getItem(c)==="true")return g(),!0;const r=document.getElementById("pagefind-ui");if(!r)return!1;r.innerHTML='<div style="padding: 2rem; text-align: center; color: var(--muted-foreground);">加载中...</div>';try{return await new Promise((o,s)=>{const a=document.createElement("script");a.src="/pagefind/pagefind-ui.js",a.onload=o,a.onerror=s,document.head.appendChild(a)}),g(),sessionStorage.setItem(c,"true"),!0}catch(i){return console.error("Failed to load search:",i),t<2?(console.log(`Retrying search load... (${t+1}/2)`),await new Promise(o=>setTimeout(o,1e3)),l(t+1)):(r&&(r.innerHTML=`
          <div style="padding: 2rem; text-align: center;">
            <p style="color: var(--muted-foreground); margin-bottom: 1rem;">搜索功能加载失败</p>
            <button id="retry-search" style="padding: 0.5rem 1rem; background: var(--accent); color: var(--accent-foreground); border: none; border-radius: 0.375rem; cursor: pointer;">
              重试
            </button>
          </div>
        `,document.getElementById("retry-search")?.addEventListener("click",()=>{sessionStorage.removeItem(c),l(0)})),!1)}}function g(){typeof PagefindUI<"u"&&(new PagefindUI({element:"#pagefind-ui",showSubResults:!0,showImages:!1,excerptLength:15,translations:{placeholder:"搜索文章...",clear_search:"清除",load_more:"加载更多",search_label:"搜索此站点",filters_label:"筛选",zero_results:"未找到结果 [SEARCH_TERM]",many_results:"找到 [COUNT] 个结果 [SEARCH_TERM]",one_result:"找到 [COUNT] 个结果 [SEARCH_TERM]",alt_search:"未找到 [SEARCH_TERM] 的结果。显示 [DIFFERENT_TERM] 的结果",search_suggestion:"未找到 [SEARCH_TERM] 的结果。尝试以下搜索：",searching:"搜索中 [SEARCH_TERM]..."}}),setTimeout(()=>{const t=document.querySelector(".pagefind-ui__search-input");t instanceof HTMLElement&&t.focus()},100))}function u(){if(d)return;d=!0;const t=document.getElementById("search-button"),n=document.getElementById("search-dialog"),r=document.getElementById("close-search");if(!t||!n||!r)return;let i=sessionStorage.getItem(c)==="true";const o=async()=>{n.showModal(),document.body.style.overflow="hidden",i?setTimeout(()=>{const e=document.querySelector(".pagefind-ui__search-input");e instanceof HTMLElement&&e.focus()},100):await l()&&(i=!0)},s=()=>{n.close(),document.body.style.overflow=""},a=e=>{e.target===n&&s()},m=e=>{e.key==="Escape"&&n.open&&s()},f=e=>{(e.metaKey||e.ctrlKey)&&e.key==="k"&&(e.preventDefault(),n.open?s():o())};t.addEventListener("click",o),r.addEventListener("click",s),n.addEventListener("click",a),document.addEventListener("keydown",m),document.addEventListener("keydown",f),document.addEventListener("astro:before-swap",()=>{t.removeEventListener("click",o),r.removeEventListener("click",s),n.removeEventListener("click",a),document.removeEventListener("keydown",m),document.removeEventListener("keydown",f),d=!1},{once:!0})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",u):u();document.addEventListener("astro:page-load",u);</script> <link href="/pagefind/pagefind-ui.css" rel="stylesheet">  </div> </div> </nav> </header>  <main id="main-content" data-pagefind-body class="main-content main-content--wide" data-astro-cid-sckkx6r4>  <div class="blog-post-container" data-astro-cid-wrgicudb> <article class="blog-post" data-astro-cid-wrgicudb> <header class="article-header" data-astro-cid-wrgicudb> <h1 data-astro-cid-wrgicudb>April 2021 newsletter</h1> <div class="article-meta" data-astro-cid-wrgicudb> <a href="https://gwern.net" target="_blank" rel="noopener noreferrer" class="source-tag" data-astro-cid-wrgicudb>gwern.net</a> <span class="separator" data-astro-cid-wrgicudb>·</span> <time data-astro-cid-wrgicudb>2021年6月3日</time> </div> </header> <!-- Render HTML content from RSS feed --> <div class="article-content app-prose" data-astro-cid-wrgicudb><p>April 2021&#8217;s <a href="https://www.gwern.net/newsletter/2021/04">Gwern.net</a> <a href="https://gwern.substack.com">newsletter</a> is now out; previous, <a href="https://www.gwern.net/newsletter/2021/03">March 2021</a> (<a href="https://www.gwern.net/tags/newsletter">archives</a>). This is a collation of links and summary of major changes, overlapping with my <a href="https://www.gwern.net/Changelog">Changelog</a>; brought to you by my donors on <a href="https://www.patreon.com/gwern">Patreon</a>.</p><h1>1 Writings</h1><ul><li><p><a href="https://www.gwern.net/Variables">Better Greek Variable Suggestions</a> (use &#1008;, &#962;, &#965;, &#982;, &#933;, &#926;, &#953;, &#1009;, &#977;, or &#928; instead)</p></li></ul><h1>2 Links</h1><h2>2.1 AI</h2><ul><li><p><a href="https://arxiv.org/abs/1810.00825">&#8220;Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks&#8221;</a>, Lee et al 2018; <a href="https://arxiv.org/abs/2103.03206#deepmind">&#8220;Perceiver: General Perception with Iterative Attention&#8221;</a>, Jaegle et al 2021 (skinny Transformers applied recurrently; given reinvention, one might ask &#8220;is <a href="https://arxiv.org/abs/1706.03762#google" title="'Attention Is All You Need', Vaswani et al 2017">attention</a>, getting too much attention?&#8221;, especially given how many Transformer tweaks <a href="https://arxiv.org/abs/2102.11972#google" title="'Do Transformer Modifications Transfer Across Implementations and Applications?', Narang et al 2021">don&#8217;t pan out</a>  or have antecedents, indicating a gold rush? Probably not: if the  marginal return on this research direction had fallen below that of  competitors, we would see those neglected directions invade Transformer  topics&#8212;while we continue to see the reverse, and many applications as  yet untouched by all the new approaches, suggesting that we <em>still</em> don&#8217;t pay enough attention)</p></li><li><p><a href="https://arxiv.org/abs/2103.04689">&#8220;Z-IL: Predictive Coding Can Do Exact Backpropagation on Any Neural Network&#8221;</a>, Salvatori et al 2021 (scaling local learning rules to ImageNet AlexNet/Resnet &amp; ALE DRL at similar compute cost)</p></li><li><p><a href="https://arxiv.org/abs/1708.07120">&#8220;Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates&#8221;</a>,  Smith &amp; Topin 2017 (the lingering mystery of super-convergence,  saving 50&#8211;90% compute with LRs as high as 20 (!): what is it, why does  it work only sometimes, is there any connection to <a href="https://www.gwern.net/docs/ai/2021-power.pdf#openai" title="'Grokking: Generalization Beyond Overfitting On Small Algorithmic Data Sets', Powers et al 2021">grokking</a> &amp; can it work for large models like GPT-3 given the <a href="https://old.reddit.com/r/MachineLearning/comments/ba1wg5/d_thoughts_about_superconvergence_and/">tunneling hypothesis</a>?)</p></li><li><p><a href="http://www.offconvex.org/2021/04/07/ripvanwinkle/">&#8220;Rip van Winkle&#8217;s Razor, a Simple New Estimate for Adaptive Data Analysis&#8221;</a>  (an unusual approach to estimating generalization&#8212;by quantifying the  information-theoretic simplicity of all the powerful DL research  discoveries since 2012, into ~1 kilobyte. And yet, <em>what</em> a kilobyte&#8230;)</p></li><li><p><a href="https://github.com/golanlevin/AmbigrammaticFigures">&#8220;Ambigrammatic Figures&#8221;</a>, Levin &amp; Huang 2020 (making horrifying StyleGAN faces that can be <a href="https://en.wikipedia.org/wiki/Ambigram">rotated 180&#176;</a> by projection &amp; then <a href="https://www.gwern.net/Faces#reversing-stylegan-to-control-modify-images">gradient-ascent</a> towards an upside-down face)</p></li></ul><p><a href="https://old.reddit.com/r/mlscaling/">Matters Of Scale</a>:</p><ul><li><p><strong><a href="https://lair.lighton.ai/akronomicon/" title="The Akronomicon: an Extreme-Scale Leaderboard">Large Models</a></strong>:</p><ul><li><p>Congratulations to OpenAI on 1 year of GPT-3 &amp; OA API. Has it really only been a year?&#8212;it has truly exceeded expectations.</p></li><li><p><a href="https://en.wikipedia.org/wiki/Naver">Naver</a> announces 204b-parameter Korean-language NN, <a href="http://m.koreaherald.com/view.php?ud=20210525000824">&#8220;HyperCLOVA&#8221;</a>  (KO; unknown arch although apparently dense, or training-compute or  benchmark/loss performance; 650b token training dataset. Who knew Naver  was even trying? &#8220;And we are here as on a darkling plain / Swept with  confused alarms of struggle and flight, / Where ignorant armies clash by  night.&#8221;)</p></li><li><p><a href="https://arxiv.org/abs/2104.12369#huawei">&#8220;PanGu-&#945;: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation&#8221;</a>,  Zeng et al 2021 (Zh; Huawei&#8217;s GPT-3-200b prototype, trained on  indigenous Chinese GPU+DL stack; a partial replication, due to  incomplete training on ~43b tokens; the <a href="https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha#user-content-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD">13b-parameter</a> model checkpoint has been released for download, and they are considering releasing the 200b-parameter model&#8230; <a href="https://chinai.substack.com/p/chinai-141-the-pangu-origin-story">Ding commentary</a>)</p></li><li><p>New &#119978;(100b)-parameter Transformer models announced at Google I/O &#8217;2021: <a href="https://blog.google/technology/ai/lamda/" title="LaMDA: our breakthrough conversation technology">LaMDA</a> (EN; chatbot), <a href="https://blog.google/products/search/introducing-mum/">MUM</a> (multimodal multilingual search/translation/Q&amp;A)</p></li><li><p><a href="https://www.infoq.cn/article/EFIHo75sQsVqLvFTruKE#alibaba">&#8220;PLUG&#8221;</a> (Zh): a 27b parameter BERT-like Chinese language model, targeting 200b next (AliBaba followup to <a href="https://arxiv.org/abs/1908.04577#alibaba" title="'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding', Wang et al 2019">StructBERT</a>/<a href="https://arxiv.org/abs/2004.07159#alibaba" title="'PALM: Pre-training an Autoencoding&amp;Autoregressive Language Model for Context-conditioned Generation', Bi et al 2020">PALM</a>)</p></li><li><p><a href="https://arxiv.org/abs/2105.13290">&#8220;CogView: Mastering Text-to-Image Generation via Transformers&#8221;</a>, Ding et al 2021 (another Chinese <a href="https://openai.com/blog/dall-e/">DALL&#183;E</a> clone, post-<a href="https://arxiv.org/abs/2103.00823#alibaba" title="'M6: A Chinese Multimodal Pretrainer', Lin et al 2021">M6</a>: <em>n</em> = <a href="https://wudaoai.cn/data-detail/1" title="WuDaoCorpus: the largest Chinese corpus data set, with about 2TB of text and 725 billion Chinese characters">30m text-image pairs</a>, 4b-parameter GPT, models to be released)</p></li><li><p><a href="https://arxiv.org/abs/2104.10157">&#8220;VideoGPT: Video Generation using VQ-VAE and Transformers&#8221;</a>, Yan et al 2021; <a href="https://arxiv.org/abs/2104.14806#microsoft">&#8220;GODIVA: </a><em><a href="https://arxiv.org/abs/2104.14806#microsoft">G</a></em><a href="https://arxiv.org/abs/2104.14806#microsoft">enerating </a><em><a href="https://arxiv.org/abs/2104.14806#microsoft">O</a></em><a href="https://arxiv.org/abs/2104.14806#microsoft">pen-</a><em><a href="https://arxiv.org/abs/2104.14806#microsoft">D</a></em><a href="https://arxiv.org/abs/2104.14806#microsoft">oma</a><em><a href="https://arxiv.org/abs/2104.14806#microsoft">I</a></em><a href="https://arxiv.org/abs/2104.14806#microsoft">n </a><em><a href="https://arxiv.org/abs/2104.14806#microsoft">V</a></em><a href="https://arxiv.org/abs/2104.14806#microsoft">ideos from n</a><em><a href="https://arxiv.org/abs/2104.14806#microsoft">A</a></em><a href="https://arxiv.org/abs/2104.14806#microsoft">tural Descriptions&#8221;</a>, Wu et al 2021 (DALL&#183;E for video on Howto100M: <a href="https://arxiv.org/abs/1906.00446#deepmind" title="'Generating Diverse High-Fidelity Images with VQ-VAE-2', Razavi et al 2019">VQ-VAE</a> + sparse attention)</p></li><li><p><a href="https://arxiv.org/abs/2104.04473#nvidia">&#8220;Efficient Large-Scale Language Model Training on GPU Clusters&#8221;</a>, Narayanan et al 2021 (Nvidia <a href="https://github.com/nvidia/megatron-lm">&#8216;Megatron-LM&#8217; software</a> for scaling up to 3072 A100 GPUs; allows 1t-parameter models at 502 petaFLOP/s or 50% efficiency, cf TPU rival, <a href="https://arxiv.org/abs/2105.04663#google" title="'GSPMD: General and Scalable Parallelization for ML Computation Graphs', Xu et al 2021: '50% to 62% compute utilization on 128 to 2048 Cloud TPUv3 cores for models with up to one trillion parameters'">GSPMD</a>, and note <a href="file:///tmp/burlyHGiKo.html#patterson-et-al-2021">Patterson et al 2021</a> estimates GPT-3 at ~3.5m V100 GPU-hours, so OA got ~20% efficiency?); <a href="https://www.youtube.com/watch?v=eAn_oiZwUXA&amp;t=2998s" title="GTC 2021 Keynote with NVIDIA CEO Jensen Huang: NVIDIA CEO Jensen Huang delivers the #GTC21&#8203; keynote, where he introduced amazing breakthroughs in building virtual worlds with NVIDIA Omniverse; in advancing enterprise computing with new NVIDIA DGX systems and software; in turning the data center into the new unit of computing with the new NVIDIA Grace CPU, BlueField-3 DPU, and DOCA 1.0 SDK; in broadening the reach of AI to all companies and industries with NVIDIA EGX and Aerial 5G; and in transforming transportation with NVIDIA DRIVE Orin and Atlan.">&#8220;We expect to see multi-trillion-parameter models by next year, and 100 trillion+ parameter models by 2023&#8221;</a> &#8212;Nvidia CEO <a href="https://en.wikipedia.org/wiki/Jensen_Huang">Jensen Huang</a> (<a href="https://www.gwern.net/docs/ai/2021-04-12-jensenhuang-gtc2021keynote-eAn_oiZwUXA.en.vtt.txt">subtitles</a>)</p></li><li><p>Mixture-Of-Experts:</p><ul><li><p><a href="https://en.pingwest.com/a/8693">BAAI&#8217;s &#8220;Wudao Wensu&#8221;: 1.75-trillion parameters &amp; multimodal!</a> (<a href="https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/">prologue</a>)</p></li><li><p><a href="https://arxiv.org/abs/2105.15082#alibaba">&#8220;Exploring Sparse Expert Models and Beyond&#8221;</a>, Yang et al 2021 (1t-parameter hierarchical Switch Transformer trained on 480 V100 GPUs)</p></li></ul></li></ul></li><li><p><strong><a href="https://arxiv.org/abs/1911.08265#deepmind">MuZero</a></strong>:</p><ul><li><p><a href="https://arxiv.org/abs/2104.06294#deepmind">&#8220;MuZero Unplugged: Online and Offline Reinforcement Learning by Planning with a Learned Model&#8221;</a>, Schrittwieser et al 2021 (Reanalyze+MuZero; <a href="https://www.gwern.net/images/ai/2021-schrittwieser-figure1-mspacmanmuzerologrewardscaling.png" title="Figure 1: Final scores in Ms. Pac-Man for different Reanalyse fractions. By scaling the Reanalyse fraction, MuZero can be trained at any desired data budget. All other parameters are held constant. Note the logarithmic x-axis: Linear improvements in score require exponentially more data, matching scaling laws such as described by Kaplan et al 2020 for language models.">smooth log-scaling</a> of <em>Ms.&nbsp;Pacman</em> reward with sample size, 107&#8211;1010, showing that DRL for arcade games parallels board games)</p></li><li><p><a href="https://sites.google.com/berkeley.edu/decision-transformer">&#8220;Decision Transformer: Reinforcement Learning via Sequence Modeling&#8221;</a>, Chen et al 2021</p></li><li><p><a href="https://arxiv.org/abs/2104.06303#deepmind">&#8220;Sampled MuZero: Learning and Planning in Complex Action Spaces&#8221;</a>, Hubert et al 2021 (MuZero for continuous domains: DM Control Suite/Real-World RL Suite); <a href="https://arxiv.org/abs/2006.07430">&#8220;Continuous Control for Searching and Planning with a Learned Model&#8221;</a>, Yang et al 2020</p></li><li><p><a href="https://arxiv.org/abs/2104.06159">&#8220;Muesli: Combining Improvements in Policy Optimization&#8221;</a>, Hessel et al 2020 (catching up with original MuZero)</p></li><li><p><a href="https://arxiv.org/abs/2102.12924">&#8220;Visualizing MuZero Models&#8221;</a>, de Vries et al 2021 (reimplementing &amp; introspecting a MuZero)</p></li></ul></li><li><p><a href="https://arxiv.org/abs/2104.03113">&#8220;Scaling Scaling Laws with Board Games&#8221;</a>, <a href="https://andyljones.com/">Jones</a> 2021 (AlphaZero/<a href="https://en.wikipedia.org/wiki/Hex_(board_game)">Hex</a>: <a href="https://www.gwern.net/notes/Faster">highly-optimized</a> GPU implementation enables showing <a href="https://www.gwern.net/notes/Scaling">smooth scaling</a>  across 6 OOM of compute&#8212;2&#215; FLOPS = 66% victory; amortization of  training &#8594; runtime tree-search, where 10&#215; training = 15&#215; runtime)</p></li><li><p><a href="https://christina.kim/2021/04/11/scaling-laws-for-language-transfer-learning/#openai">&#8220;Scaling Laws for Language Transfer Learning&#8221;</a>, Christina Kim (<a href="https://arxiv.org/abs/2102.01293#openai" title="Scaling Laws for Transfer">Hernandez et al 2021</a> followup: smooth scaling for En &#8594; De/Es/Zh)</p></li><li><p><a href="https://arxiv.org/abs/2104.10350#google">&#8220;Carbon Emissions and Large Neural Network Training&#8221;</a>,  Patterson et al 2021 (&#8220;&#8230;choice of DNN/datacenter/processor can reduce  the carbon footprint up to ~100&#8211;1000&#215;. These large factors make  retroactive estimates difficult.&#8221;)</p></li><li><p><a href="https://arxiv.org/abs/2104.07705">&#8220;How to Train BERT with an Academic Budget&#8221;</a>, Izsak et al 2021 (<a href="https://arxiv.org/abs/1810.04805#google" title="'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', Devlin et al 2018">BERT</a> in 8 GPU-days&#8212;R&amp;D iteration allows finding efficiency; there&#8217;s nothing so expensive as demanding research be cheap.^1^)</p></li></ul><h2>2.2 Genetics</h2><p>Everything Is Heritable:</p><ul><li><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6818669/">&#8220;Precision exercise medicine: understanding exercise response variability&#8221;</a>,  Ross et al 2019 (&#8220;large individual differences in CRF response (range:  &#8722;33% to +118%) have been observed across the 8 exercise training studies  independent of exercise duration&#8221;&#8212;nothing in psychology, or medicine,  makes sense except in the light of individual differences&#8230;)</p></li></ul><p>Recent Evolution:</p><ul><li><p><a href="https://academic.oup.com/mbe/advance-article/doi/10.1093/molbev/msab147/6277411">&#8220;Analysis of genomic DNA from medieval plague victims suggests long-term effect of </a><em><a href="https://academic.oup.com/mbe/advance-article/doi/10.1093/molbev/msab147/6277411">Yersinia pestis</a></em><a href="https://academic.oup.com/mbe/advance-article/doi/10.1093/molbev/msab147/6277411"> on human immunity genes&#8221;</a>, Immel et al 2021</p></li></ul><p>Engineering:</p><ul><li><p><a href="https://biohackinfo.com/news-china-gene-editing-criminal-law-article-336-march-2021/">&#8220;China officially bans CRISPR babies, human clones and animal-human hybrids&#8221;</a>? (another blow to attempts to project fears &amp; fantasies onto China)</p></li></ul><h2>2.3 Politics/Religion</h2><ul><li><p><em><a href="https://www.nap.edu/catalog/25762/reflecting-sunlight-recommendations-for-solar-geoengineering-research-and-research-governance">Reflecting Sunlight: Recommendations for Solar Geoengineering Research and Research Governance</a></em>, National Academies 2021 (<a href="https://www.nytimes.com/2021/03/25/climate/geoengineering-sunlight.html">media</a>)</p></li><li><p><a href="https://www.gwern.net/docs/sociology/2020-muralidharan.pdf">&#8220;Improving Public Sector Management at Scale? Experimental Evidence on School Governance India&#8221;</a>, Muralidharan &amp; Singh 2020</p></li><li><p><a href="https://www.gwern.net/docs/fiction/2012-mason.pdf">&#8220;Jay-Z&#8217;s </a><em><a href="https://www.gwern.net/docs/fiction/2012-mason.pdf">99 Problems</a></em><a href="https://www.gwern.net/docs/fiction/2012-mason.pdf">, Verse 2: A Close Reading with 4th Amendment Guidance for Cops and Perps&#8221;</a>, Mason 2012</p></li></ul><h2>2.4 Psychology/Biology</h2><ul><li><p><a href="https://www.gwern.net/docs/longevity/2021-wiley.pdf">&#8220;Oxylipin biosynthesis reinforces cellular senescence and allows detection of senolysis&#8221;</a>, Wiley et al 2021</p></li><li><p><a href="https://www.nytimes.com/2019/02/26/magazine/psychics-skeptics-facebook.html" title="Are some celebrity mediums fooling their audience members by reading social media pages in advance? A group of online vigilantes is out to prove it">&#8220;Inside the Secret Sting Operations to Expose Celebrity Psychics&#8221;</a></p></li><li><p><a href="https://www.gwern.net/docs/catnip/2021-smith.pdf">&#8220;If I fits I sits: A citizen science investigation into illusory contour susceptibility in domestic cats (</a><em><a href="https://www.gwern.net/docs/catnip/2021-smith.pdf">Felis silvestris catus</a></em><a href="https://www.gwern.net/docs/catnip/2021-smith.pdf">)&#8221;</a>, Smith et al 2021</p></li><li><p><a href="https://www.gwern.net/docs/biology/2005-paxton.pdf">&#8220;Cetaceans,  sex and sea serpents: an analysis of the Egede accounts of a &#8216;most  dreadful monster&#8217; seen off the coast of Greenland in 1734&#8221;</a>, Paxton et al 2005 (is that a legendary cryptid in your pocket, or are you just happy to see me?)</p></li><li><p><a href="https://www.gwern.net/docs/psychology/writing/2020-reilly.pdf">&#8220;Building the perfect curse word: A psycholinguistic investigation of the form and meaning of taboo words&#8221;</a>, Reilly et al 2020</p></li><li><p><a href="https://en.wikipedia.org/wiki/Tarrare">Tarrare</a></p></li></ul><h2>2.5 Technology</h2><ul><li><p><a href="https://arxiv.org/abs/2103.07487">&#8220;How Developers Choose Names&#8221;</a>,  Feitelson et al 2021 (&#8220;Another example concerned the function  &#8216;arrangeFilesByName(files)&#8217;. When asked the return value&#8230;one suggested  the number of files reordered&#8221;)</p></li><li><p><a href="https://arxiv.org/abs/2004.02504">&#8220;Bringing GNU Emacs to Native Code&#8221;</a>,  Corallo et al 2020 (using libgccjit to make Emacs 2.3&#215; to 42&#215; faster;  gccemacs has been merged into Emacs HEAD &amp; will be available soon)</p></li><li><p><a href="https://phiresky.github.io/blog/2021/hosting-sqlite-databases-on-github-pages/">&#8220;Hosting SQLite databases on Github Pages (or any static file hoster)&#8221;</a> (a revolution in static website technology: eg running a query <a href="https://nitter.cc/simonw/status/1388933800445452290" title="Check out this demo: I run the SQL query &quot;select country_code, long_name from wdi_country order by rowid desc limit 100&quot; and it fetches just 54.2KB of new data (across 49 small HTTP requests) to return 100 results---from a statically hosted database file that's 668.8MB!">need download only 54kb of a 670MB database</a>; fulltext site search is just the beginning of the possibilities of this clever use of <a href="https://en.wikipedia.org/wiki/Byte_serving">range requests</a>)</p></li><li><p><a href="https://www.coderelay.io/fontemon.html">&#8220;</a><em><a href="https://www.coderelay.io/fontemon.html">Fontemon</a></em><a href="https://www.coderelay.io/fontemon.html">: World&#8217;s first video game in a font!&#8221;</a> (a <em>Pokemon</em>-like CYOA <a href="https://github.com/mmulet/code-relay/blob/main/markdown/HowIDidIt.md">implemented as an OpenType font file</a>; play in browser or text editor&#8212;still not quite <a href="https://www.gwern.net/Turing-complete">Turing-complete</a> but definitely the most impressive thing implemented in a font so far)</p><ul><li><p><em>Fontemon</em> is by far the highlight of <a href="http://sigbovik.org/2021/proceedings.pdf">SIGBOVIK 2021</a>; but also worth noting: <a href="http://sigbovik.org/2021/proceedings.pdf#page=8">&#8220;Back to Square One: Superhuman Performance in Chutes and Ladders Through Deep Neural Networks and Tree Search&#8221;</a> &#183; <a href="http://sigbovik.org/2021/proceedings.pdf#page=83">&#8220;Deep Deterministic Policy Gradient Boosted Decision Trees&#8221;</a> &#183; <a href="http://sigbovik.org/2021/proceedings.pdf#page=126">&#8220;Lowestcase and uppestcase letters: Advances in derp learning&#8221;</a> &#183; <a href="http://sigbovik.org/2021/proceedings.pdf#page=167">&#8220;openCHEAT: Computationally Helped Error bar Approximation Tool&#8212;Kick-starting Science 4.0&#8221;</a> &#183; <a href="http://sigbovik.org/2021/proceedings.pdf#page=216">&#8220;The Newcomb-Benford Law, Applied to Binary Data: An Empirical and Theoretic Analysis&#8221;</a> &#183; <a href="http://sigbovik.org/2021/proceedings.pdf#page=252">&#8220;Inverted Code Theory: Manipulating Program Entropy&#8221;</a> (<em><a href="https://en.wikipedia.org/wiki/Tenet_(film)">Tenet</a></em> fans only&#8212;possibly inferior to <a href="http://www.frc.ri.cmu.edu/~hpm/project.archive/general.articles/1991/TempComp.html" title="Time Travel and Computing">Moravec 1991</a>?) &#183; <a href="http://sigbovik.org/2021/proceedings.pdf#page=282">&#8220;Build your own 8-bit busy beaver on a breadboard!&#8221;</a></p></li></ul><p>Incidentally, it&#8217;s curious that while STEM fields have entire annual issues, journals, &amp; conferences devoted to satire (<a href="http://sigbovik.org/">SIGBOVIK</a>; Arxiv April Fools papers like <a href="https://arxiv.org/abs/1703.10987" title="On the Impossibility of Supersized Machines">Garfinkel et al 2017</a>; <a href="https://www108.lamp.le.ac.uk/ojs1/index.php/pst/issue/archive">Special Topics</a>; the <a href="https://www.bmj.com/about-bmj/resources-authors/article-types/christmas-issue">BMJ Christmas issue</a>; the <a href="https://en.wikipedia.org/wiki/Ig_Nobel_Prize">Ig Nobel Prizes</a> &amp; <a href="https://bahfest.com/">BAHFest</a>), after asking in several places, I have found no instances in the humanities. (I know of many entertaining <em>papers</em>, like <a href="https://www.gwern.net/docs/philo/2008-sinhababu.pdf" title="Possible Girls">Sinhababu 2008</a> on waifus, but no <em>regular organized</em> publication, with the possible exception of the annual <a href="https://en.wikipedia.org/wiki/Latke%E2%80%93Hamantash_Debate">&#8220;Latke-Hamantash Debate&#8221;</a>.)</p></li></ul><h2>2.6 Economics</h2><ul><li><p><a href="https://www.gwern.net/docs/statistics/decision/2006-thorp.pdf">&#8220;The Kelly Criterion in Blackjack Sports Betting, and the Stock Market&#8221;</a>, Thorp 2006</p></li><li><p><a href="https://marginalrevolution.com/marginalrevolution/2016/10/performance-pay-nobel.html">&#8220;The Performance Pay Nobel&#8221;</a> (CEO pay as <a href="https://www.gwern.net/Backstop">blackbox optimization problem</a>)</p></li><li><p><a href="https://www.gwern.net/docs/economics/2008-josephson.pdf">&#8220;The Ocean&#8217;s Hot Dog: The Development of the Fish Stick&#8221;</a>,  Kelly 2008 (out of nostalgia, I bought some fish sticks for the first  time in decades; better than I remembered, even if I had no <a href="https://en.wikipedia.org/wiki/Tartar_sauce">tartar</a> handy)</p></li></ul><h2>2.7 Philosophy</h2><ul><li><p><a href="https://www.gwern.net/docs/culture/2007-shiner.pdf">&#8220;The Aesthetics of Smelly Art&#8221;</a>, Shiner &amp; Kriskovets 2007; <a href="https://www.gwern.net/docs/culture/2019-kraft.pdf">&#8220;The Odor Value Concept in the Formal Analysis of Olfactory Art&#8221;</a>, Kraft 2019; <a href="https://qualiacomputing.com/2020/02/21/perfumery-as-an-art-form/" title="Hedonic Tone, memetics, scent, sex, spirituality">&#8220;Perfumery as an art form&#8221;</a>/<a href="https://qualiacomputing.com/2020/08/14/qualia-research-diary-scents/" title="Qualia Research Diary: Scents [consciousness research, Experiment, genetics, memetics, scent, valence]">notes</a>, Qualia Computing 2020 (more: manufacturing: <a href="https://www.newyorker.com/magazine/2005/03/14/scent-nile" title="Chandler Burr 2005">&#8220;The Scent of the Nile: Jean-Claude Ellena creates a new perfume&#8221;</a>; human smell is better than you think: <a href="https://www.gwern.net/docs/psychology/2006-porter.pdf">&#8220;Mechanisms of Scent-tracking in Humans&#8221;</a>, Porter et al 2006 (<a href="https://www.gwern.net/images/psychology/2006-porter-humanscenttracking-41593_2007_bfnn1819_moesm2_esm.mp4">video</a>; see also <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5512720/">&#8220;Poor Human Olfaction is a 19th Century Myth&#8221;</a>, McGann 2017); <a href="https://www.pnas.org/content/109/49/19959.full" title="'Perceptual convergence of multi-component mixtures in olfaction implies an olfactory white', Weiss et al 2012">olfactory white</a>; <em><a href="https://en.wikipedia.org/wiki/K%C5%8Dd%C5%8D">K&#333;d&#333;</a></em>, which unexpectedly appears in <a href="https://www.gwern.net/docs/cs/2005-knuth-taocp-v4-prefascicle4b.pdf#page=22" title="7.2.1.7: History of Combinatorial Generation: Set Partitions">Knuth</a>. <a href="https://threadreaderapp.com/thread/1357071738731814912.html" title="https://twitter.com/add_hawk/status/1357071738731814912">C. Thi Nguyen</a>&#8217;s description of the more bizarre &amp; avant-garde perfumes made me curious enough to nose around &amp; order 39 <a href="https://www.luckyscent.com/">LuckyScent</a> samplers.)</p></li></ul><h2>2.8 Miscellaneous</h2><ul><li><p><a href="https://en.wikipedia.org/wiki/Bog_butter">Bog butter</a></p></li><li><p><a href="https://en.wikipedia.org/wiki/Sarah_Bernhardt">Sarah Bernhardt</a> (Lions. Lots of lions.)</p></li></ul><div><hr></div><ol><li><p>Another thought, looking at <a href="https://bls.gov/news.release/ecec.nr0.htm">&#8216;Employer Costs for Employee Compensation&#8217;</a> (<a href="https://bls.gov/news.release/archives/ecec_031986.pdf">PDF</a>):</p><ol><li><p>&#8220;Moore&#8217;s Law&#8221;: the cost of a transistor halves every ~19 months;</p></li><li><p>&#8220;Anti-Moore&#8217;s Law&#8221;: the cost of a synapse doubles every ~119 years.</p></li></ol></li></ol></div> <footer class="article-footer" data-astro-cid-wrgicudb> <div class="action-row" data-astro-cid-wrgicudb> <a href="https://gwern.substack.com/p/april-2021-newsletter" target="_blank" rel="noopener noreferrer" class="read-original-btn" data-astro-cid-wrgicudb>
阅读原文 ↗
</a> </div> <div class="back-to-list" data-astro-cid-wrgicudb> <a href="/reading-list" class="back-link" data-astro-cid-wrgicudb>← 返回订阅列表</a> </div> </footer> </article> </div>  </main> <footer class="site-footer" aria-label="Site footer" data-astro-cid-sz7xmlte> <div class="footer-content" data-astro-cid-sz7xmlte> <p class="footer-text" data-astro-cid-sz7xmlte>
&copy; 2026 AstroBlog
</p> <p class="footer-text" data-astro-cid-sz7xmlte>
Built with  <a href="https://astro.build/" target="_blank" rel="noopener noreferrer" class="footer-link" data-astro-cid-sz7xmlte>
Astro
</a> </p> </div> </footer>  </body></html> 