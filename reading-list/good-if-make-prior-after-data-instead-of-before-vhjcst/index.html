<!DOCTYPE html><html lang="zh-CN" data-astro-cid-sckkx6r4> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v5.17.1"><meta name="description" content="据说你应该事先选择你的先验。这就是为什么它被称为“先验”。首先，你应该说明不同事物的可能性，然后根据数据更新你的信念。"><meta http-equiv="Content-Security-Policy" content="default-src 'self'; style-src 'self' 'unsafe-inline'; script-src 'self' 'unsafe-inline'; img-src 'self' data: https:; connect-src 'self'; font-src 'self' data:;"><link rel="canonical" href="https://blog.yuyins.com/reading-list/good-if-make-prior-after-data-instead-of-before-vhjcst/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://blog.yuyins.com/reading-list/good-if-make-prior-after-data-instead-of-before-vhjcst/"><meta property="og:title" content="如果先收集数据再制定先验会更好"><meta property="og:description" content="据说你应该事先选择你的先验。这就是为什么它被称为“先验”。首先，你应该说明不同事物的可能性，然后根据数据更新你的信念。"><meta property="og:image" content="https://blog.yuyins.com/favicon.svg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://blog.yuyins.com/reading-list/good-if-make-prior-after-data-instead-of-before-vhjcst/"><meta property="twitter:title" content="如果先收集数据再制定先验会更好"><meta property="twitter:description" content="据说你应该事先选择你的先验。这就是为什么它被称为“先验”。首先，你应该说明不同事物的可能性，然后根据数据更新你的信念。"><meta property="twitter:image" content="https://blog.yuyins.com/favicon.svg"><title>如果先收集数据再制定先验会更好</title><!-- Theme and color scheme script (runs before page loads to prevent flash) --><script>
			// 初始化配色方案 - 默认使用 stone-amber
			const colorScheme = localStorage.getItem('colorScheme') || 'stone-amber';
			document.documentElement.setAttribute('data-color', colorScheme);

			// 强制使用浅色主题
			document.documentElement.setAttribute('data-theme', 'light');
		</script><link rel="stylesheet" href="/_astro/_slug_.AC5XNt97.css">
<style>.blog-post-container[data-astro-cid-wrgicudb]{width:100%;margin:0 auto;padding:2rem 0}.blog-post[data-astro-cid-wrgicudb]{width:100%;min-width:0;max-width:42rem;margin:0 auto}.article-header[data-astro-cid-wrgicudb]{margin-bottom:2rem;padding-bottom:1rem;border-bottom:1px solid var(--border)}.article-header[data-astro-cid-wrgicudb] h1[data-astro-cid-wrgicudb]{font-size:2rem;line-height:1.3;margin-bottom:1rem;font-weight:700;color:var(--foreground);word-break:break-word}.article-meta[data-astro-cid-wrgicudb]{display:flex;align-items:center;gap:.5rem;font-size:.875rem;color:var(--muted-foreground)}.source-tag[data-astro-cid-wrgicudb]{background-color:var(--muted);color:var(--foreground);padding:.125rem .5rem;border-radius:9999px;font-weight:500;font-size:.75rem;text-decoration:none;transition:opacity .2s}.source-tag[data-astro-cid-wrgicudb]:hover{opacity:.8}.separator[data-astro-cid-wrgicudb]{color:var(--border)}.article-content[data-astro-cid-wrgicudb]{line-height:1.8;margin-bottom:3rem;font-size:1.0625rem;color:var(--foreground)}.article-content[data-astro-cid-wrgicudb] h2{margin-top:2rem;margin-bottom:1rem;font-size:1.5rem;font-weight:600}.article-content[data-astro-cid-wrgicudb] h3{margin-top:1.5rem;margin-bottom:.75rem;font-size:1.25rem;font-weight:600}.article-content[data-astro-cid-wrgicudb] p{margin-bottom:1.25rem}.article-content[data-astro-cid-wrgicudb] a{color:var(--accent);text-decoration:underline;text-underline-offset:2px}.article-content[data-astro-cid-wrgicudb] img{max-width:100%;height:auto;border-radius:.5rem;margin:1.5rem 0}.article-content[data-astro-cid-wrgicudb] blockquote{border-left:4px solid var(--border);padding-left:1rem;margin:1.5rem 0;color:var(--muted-foreground);font-style:italic}.article-content[data-astro-cid-wrgicudb] pre{background:var(--muted);padding:1rem;border-radius:.5rem;overflow-x:auto;font-family:monospace;margin:1.5rem 0}.article-content[data-astro-cid-wrgicudb] ul,.article-content[data-astro-cid-wrgicudb] ol{padding-left:1.5rem;margin-bottom:1.25rem}.article-content[data-astro-cid-wrgicudb] li{margin-bottom:.5rem}.article-footer[data-astro-cid-wrgicudb]{padding-top:2rem;border-top:1px solid var(--border);display:flex;flex-direction:column;gap:1.5rem;align-items:center}.action-row[data-astro-cid-wrgicudb]{display:flex;justify-content:center}.read-original-btn[data-astro-cid-wrgicudb]{display:inline-flex;align-items:center;gap:.5rem;padding:.75rem 1.5rem;background-color:var(--foreground);color:var(--background);border-radius:.5rem;font-weight:500;text-decoration:none;transition:opacity .2s}.read-original-btn[data-astro-cid-wrgicudb]:hover{opacity:.9}.back-to-list[data-astro-cid-wrgicudb]{text-align:center}.back-link[data-astro-cid-wrgicudb]{color:var(--muted-foreground);text-decoration:none;font-size:.875rem;transition:color .2s}.back-link[data-astro-cid-wrgicudb]:hover{color:var(--foreground)}
</style></head> <body data-astro-cid-sckkx6r4> <a id="skip-to-content" href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:top-4 focus:left-4 focus:z-50 focus:px-4 focus:py-2 focus:bg-accent focus:text-background" data-astro-cid-sckkx6r4>Skip to content</a> <header class="header" data-astro-cid-3ef6ksr2> <nav class="header-nav" aria-label="Main navigation" data-astro-cid-3ef6ksr2> <a href="/" class="header-logo" data-astro-cid-3ef6ksr2>
AstroBlog
</a> <div class="header-links" data-astro-cid-3ef6ksr2> <ul class="nav-list" data-astro-cid-3ef6ksr2> <li data-astro-cid-3ef6ksr2> <a href="/" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 文章 </a> </li><li data-astro-cid-3ef6ksr2> <a href="/category" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 分类 </a> </li><li data-astro-cid-3ef6ksr2> <a href="/reading-list" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 订阅 </a> </li><li data-astro-cid-3ef6ksr2> <a href="/about" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 关于 </a> </li> </ul> <div class="header-actions" data-astro-cid-3ef6ksr2> <div id="search" data-astro-cid-otpdt6jm> <button id="search-button" class="search-toggle" aria-label="打开搜索" title="搜索 (⌘K)" data-astro-cid-otpdt6jm> <svg class="search-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-astro-cid-otpdt6jm> <circle cx="11" cy="11" r="7" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-otpdt6jm></circle> <path d="M20 20L16.5 16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-otpdt6jm></path> </svg> </button> <dialog id="search-dialog" class="search-modal" data-astro-cid-otpdt6jm> <div class="modal-content" data-astro-cid-otpdt6jm> <div class="modal-header" data-astro-cid-otpdt6jm> <h2 class="modal-title" data-astro-cid-otpdt6jm>搜索文章</h2> <button id="close-search" class="close-button" aria-label="关闭搜索" data-astro-cid-otpdt6jm> <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-astro-cid-otpdt6jm> <path d="M18 6L6 18M6 6l12 12" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" data-astro-cid-otpdt6jm></path> </svg> </button> </div> <div id="pagefind-ui" data-astro-cid-otpdt6jm></div> </div> </dialog> </div> <script type="module">let d=!1;const c="pagefind-loaded";async function l(t=0){if(sessionStorage.getItem(c)==="true")return g(),!0;const r=document.getElementById("pagefind-ui");if(!r)return!1;r.innerHTML='<div style="padding: 2rem; text-align: center; color: var(--muted-foreground);">加载中...</div>';try{return await new Promise((o,s)=>{const a=document.createElement("script");a.src="/pagefind/pagefind-ui.js",a.onload=o,a.onerror=s,document.head.appendChild(a)}),g(),sessionStorage.setItem(c,"true"),!0}catch(i){return console.error("Failed to load search:",i),t<2?(console.log(`Retrying search load... (${t+1}/2)`),await new Promise(o=>setTimeout(o,1e3)),l(t+1)):(r&&(r.innerHTML=`
          <div style="padding: 2rem; text-align: center;">
            <p style="color: var(--muted-foreground); margin-bottom: 1rem;">搜索功能加载失败</p>
            <button id="retry-search" style="padding: 0.5rem 1rem; background: var(--accent); color: var(--accent-foreground); border: none; border-radius: 0.375rem; cursor: pointer;">
              重试
            </button>
          </div>
        `,document.getElementById("retry-search")?.addEventListener("click",()=>{sessionStorage.removeItem(c),l(0)})),!1)}}function g(){typeof PagefindUI<"u"&&(new PagefindUI({element:"#pagefind-ui",showSubResults:!0,showImages:!1,excerptLength:15,translations:{placeholder:"搜索文章...",clear_search:"清除",load_more:"加载更多",search_label:"搜索此站点",filters_label:"筛选",zero_results:"未找到结果 [SEARCH_TERM]",many_results:"找到 [COUNT] 个结果 [SEARCH_TERM]",one_result:"找到 [COUNT] 个结果 [SEARCH_TERM]",alt_search:"未找到 [SEARCH_TERM] 的结果。显示 [DIFFERENT_TERM] 的结果",search_suggestion:"未找到 [SEARCH_TERM] 的结果。尝试以下搜索：",searching:"搜索中 [SEARCH_TERM]..."}}),setTimeout(()=>{const t=document.querySelector(".pagefind-ui__search-input");t instanceof HTMLElement&&t.focus()},100))}function u(){if(d)return;d=!0;const t=document.getElementById("search-button"),n=document.getElementById("search-dialog"),r=document.getElementById("close-search");if(!t||!n||!r)return;let i=sessionStorage.getItem(c)==="true";const o=async()=>{n.showModal(),document.body.style.overflow="hidden",i?setTimeout(()=>{const e=document.querySelector(".pagefind-ui__search-input");e instanceof HTMLElement&&e.focus()},100):await l()&&(i=!0)},s=()=>{n.close(),document.body.style.overflow=""},a=e=>{e.target===n&&s()},m=e=>{e.key==="Escape"&&n.open&&s()},f=e=>{(e.metaKey||e.ctrlKey)&&e.key==="k"&&(e.preventDefault(),n.open?s():o())};t.addEventListener("click",o),r.addEventListener("click",s),n.addEventListener("click",a),document.addEventListener("keydown",m),document.addEventListener("keydown",f),document.addEventListener("astro:before-swap",()=>{t.removeEventListener("click",o),r.removeEventListener("click",s),n.removeEventListener("click",a),document.removeEventListener("keydown",m),document.removeEventListener("keydown",f),d=!1},{once:!0})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",u):u();document.addEventListener("astro:page-load",u);</script> <link href="/pagefind/pagefind-ui.css" rel="stylesheet">  </div> </div> </nav> </header>  <main id="main-content" data-pagefind-body class="main-content main-content--wide" data-astro-cid-sckkx6r4>  <div class="blog-post-container" data-astro-cid-wrgicudb> <article class="blog-post" data-astro-cid-wrgicudb> <header class="article-header" data-astro-cid-wrgicudb> <h1 data-astro-cid-wrgicudb>如果先收集数据再制定先验会更好</h1> <div class="article-meta" data-astro-cid-wrgicudb> <a href="https://dynomight.net" target="_blank" rel="noopener noreferrer" class="source-tag" data-astro-cid-wrgicudb>dynomight.net</a> <span class="separator" data-astro-cid-wrgicudb>·</span> <time data-astro-cid-wrgicudb>2025年12月18日</time> </div> </header> <!-- 使用清洗后的安全 HTML 内容 --> <div class="article-content app-prose" data-astro-cid-wrgicudb><p>They say you’re supposed to choose your prior in advance. That’s why it’s called a “prior”. <em>First</em>, you’re supposed to say say how plausible different things are, and <em>then</em> you update your beliefs based on what you see in the world.</p>

<p>For example, currently you are—I assume—trying to decide if you should stop reading this post and do something else with your life. If you’ve read this blog before, then lurking somewhere in your mind is some prior for how often my posts are good. For the sake of argument, let’s say you think 25% of my posts are funny and insightful and 75% are boring and worthless.</p>

<p><img src="/img/prior/example1.svg" alt="" /></p>

<p>OK. But now here you are reading these words. If they seem bad/good, then that raises the odds that this particular post is worthless/non-worthless. For the sake of argument again, say you find these words mildly promising, meaning that a good post is 1.5× more likely than a worthless post to contain words with this level of quality.</p>

<p><img src="/img/prior/example2.svg" alt="" /></p>

<p>If you combine those two assumptions, that implies that the probability that this particular post is good is 33.3%. That’s true because the red rectangle below has half the area of the blue one, and thus the probability that this post is good should be half the probability that it’s bad (33.3% vs. 66.6%)</p>

<p><img src="/img/prior/example3.svg" alt="" /></p>


  
(Why half the area? Because the red rectangle is ⅓ as wide and ³⁄₂ as tall as the blue one and ⅓ × ³⁄₂ = ½. If you only trust equations, click here for equations.)


  <p>It’s easiest to calculate the ratio of the odds that the post is good versus bad, namely</p>

  <div><div><pre><code>P[good | words] / P[bad | words]
 = P[good, words] / P[bad, words]
 = (P[good] × P[words | good])
 / (P[bad] × P[words | bad])
 = (0.25 × 1.5) / (0.75 × 1)
 = 0.5.
</code></pre></div>  </div>

  <p>It follows that</p>

  <div><div><pre><code>P[good | words] = 0.5 × P[bad | words],
</code></pre></div>  </div>

  <p>and thus that</p>

  <div><div><pre><code>P[good | words] = 1/3.
</code></pre></div>  </div>

  <p>Alternatively, if you insist on using Bayes’ equation:</p>

  <div><div><pre><code>P[good | words]
 = P[good] × P[words | good] / P[words]
 = P[good] × P[words | good]
 / (P[good] × P[words | good] + P[bad] × P[words | bad])
 = 0.25 × 1.5 / (0.25 × 1.5 + 0.75)
 = (1/3)
</code></pre></div>  </div>



<p>Theoretically, when you chose your prior that 25% of dynomight posts are good, that was supposed to reflect all the information you encountered in life <em>before</em> reading this post. Changing that number based on information contained in this post wouldn’t make any sense, because that information is supposed to be reflected in the second step when you choose your likelihood <code>p[good | words]</code>. Changing your prior based on this post would amount to “double-counting”.</p>

<p>In theory, that’s right. It’s also right in practice for the above example, and for the similar <a href="https://en.wikipedia.org/wiki/Representativeness_heuristic#The_taxicab_problem" target="_blank" rel="noopener noreferrer">cute little examples</a> you find in textbooks.</p>

<p>But for real problems, I’ve come to believe that refusing to change your prior after you see the data often leads to tragedy. The reason is that in real problems, things are rarely just “good” or “bad”, “true” or “false”. Instead, truth comes in an infinite number of varieties. And you often can’t predict which of these varieties matter until after you’ve seen the data.</p>

<h2>Aliens</h2>

<p>Let me show you what I mean. Say you’re wondering if there are aliens on Earth. As far as we know, there’s no reason aliens shouldn’t have emerged out of the random swirling of molecules on some other planet, developed a technological civilization, built spaceships, and shown up here. So it seems reasonable to choose a prior it’s equally plausible that there are aliens or that there are not, i.e. that</p>

<div><div><pre><code>P[aliens] ≈ P[no aliens] ≈ 50%.
</code></pre></div></div>

<p><img src="/img/prior/cartoon2.svg" alt="" /></p>

<p>Meanwhile, here on our actual world, we have lots of weird alien-esque evidence, like the <a href="https://en.wikipedia.org/wiki/File:Gimbal_The_First_Official_UAP_Footage_from_the_USG_for_Public_Release.webm" target="_blank" rel="noopener noreferrer">Gimbal video</a>, the <a href="https://en.wikipedia.org/wiki/File:Go_Fast_Official_USG_Footage_of_UAP_for_Public_Release.webm" target="_blank" rel="noopener noreferrer">Go Fast video</a>, the <a href="https://en.wikipedia.org/wiki/File:FLIR1_Official_UAP_Footage_from_the_USG_for_Public_Release.webm" target="_blank" rel="noopener noreferrer">FLIR1 video</a>, the <a href="https://en.wikipedia.org/wiki/Wow!_signal" target="_blank" rel="noopener noreferrer">Wow! signal</a>, government reports on <a href="https://en.wikipedia.org/wiki/UFO_Report_(U.S._Intelligence)" target="_blank" rel="noopener noreferrer">unidentified aerial phenomena</a>, and lots of pilots that report seeing “tic-tacs” fly around in physically impossible ways. Call all that stuff <code>data</code>. If aliens weren’t here, then it seems hard to explain all that stuff. So it seems like <code>P[data | no aliens]</code> should be some low number.</p>

<p>On the other hand, if aliens <em>were</em> here, then why don’t we ever get a good image? Why are there endless confusing reports and rumors and grainy videos, but <em>never</em> a single clear close-up high-resolution video, and <em>never</em> any alien debris found by some random person on the ground? That also seems hard to explain if aliens <em>were</em> here. So I think <code>P[data | aliens]</code> should also be some low number. For the sake of simplicity, let’s call it a wash and assume that</p>

<div><div><pre><code>P[data | no aliens] ≈ P[data | aliens].
</code></pre></div></div>

<p><img src="/img/prior/cartoon3.svg" alt="" /></p>

<p>Since neither the prior nor the data see any difference between aliens and no-aliens, the posterior probability is</p>

<div><div><pre><code>P[no aliens | data] ≈ P[aliens | data] ≈ 50%.
</code></pre></div></div>

<p><img src="/img/prior/cartoon4.svg" alt="" /></p>

<p>See the problem?</p>


  
(Click here for math.)


  <p>Observe that</p>

  <div><div><pre><code>P[aliens | data] / P[no aliens | data]
 = P[aliens, data] / P[no aliens, data]
 = (P[aliens] × P[data | aliens])
 / (P[no aliens] × P[data | no aliens])
 ≈ 1,
</code></pre></div>  </div>

  <p>where the last line follows from the fact that <code>P[aliens] ≈ P[no aliens]</code> and <code>P[data | aliens] ≈ P[data | no aliens]</code>. Thus we have that</p>

  <div><div><pre><code>P[aliens | data] ≈ P[no aliens | data] ≈ 50%.
</code></pre></div>  </div>



<p>We’re friends. We respect each other. So let’s not argue about if my starting assumptions are good. They’re my assumptions. I like them. And yet the final conclusion seems insane to me. What went wrong?</p>

<p>Assuming I didn’t screw up the math (I didn’t), the obvious explanation is that I’m experiencing cognitive dissonance as a result of a poor decision on my part to adopt a set of mutually contradictory beliefs. Say you claim that Alice is taller than Bob and Bob is taller than Carlos, but you deny that Alice is taller than Carlos. If so, that would mean that you’re confused, not that you’ve discovered some interesting paradox.</p>

<p>Perhaps if I believe that <code>P[aliens] ≈ P[no aliens]</code> and that <code>P[data | aliens] ≈ P[data | no aliens]</code>, then I <em>must</em> accept that <code>P[aliens | data] ≈ P[no aliens | data]</code>. Maybe rejecting that conclusion just means I have some personal issues I need to work on.</p>

<p>I deny that explanation. I deny it! Or, at least, I deny that’s it’s most helpful way to think about this situation. To see why, let’s build a second model.</p>

<h2>More aliens</h2>

<p>Here’s a trivial observation that turns out to be important: “There are aliens” isn’t a single thing. There could be furry aliens, slimy aliens, aliens that like synthwave music, etc. When I stated my prior, I could have given different probabilities to each of those cases. But if I had, it wouldn’t have changed anything, because there’s no reason to think that furry vs. slimy aliens would have any difference in their eagerness to travel to ape-planets and fly around in physically impossible tic-tacs.</p>

<p>But suppose I had divided up the state of the world into these four possibilities:</p>

<table>
  <thead>
    <tr>
      <th>possibility</th>
      <th>description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>No aliens + normal people</code></td>
      <td>There are no aliens. Meanwhile, people are normal and not prone to hallucinating evidence for things that don’t exist.</td>
    </tr>
    <tr>
      <td><code>No aliens + weird people</code></td>
      <td>There are no aliens. Meanwhile, people are weird and <em>do</em> tend to hallucinate evidence for things that don’t exist.</td>
    </tr>
    <tr>
      <td><code>Normal aliens</code></td>
      <td>There are aliens. They may or may not have cool spaceships or enjoy shooting people with lasers. But one way or another, they leave obvious, indisputable evidence that they’re around.</td>
    </tr>
    <tr>
      <td><code>Weird aliens</code></td>
      <td>There are aliens. But they stay hidden until humans get interested in space travel. And after that, they let humans take confusing grainy videos, but never a single good video, never ever, not one.</td>
    </tr>
  </tbody>
</table>

<p>If I had broken things down that way, I might have chosen this prior:</p>

<div><div><pre><code>P[no aliens + normal people] ≈ 41%
P[no aliens + weird people] ≈ 9%
P[normal aliens] ≈ 49%
P[weird aliens] ≈ 1%
</code></pre></div></div>

<p><img src="/img/prior/cartoon5.svg" alt="" /></p>

<p>Now, let’s think about the empirical evidence again. It’s incompatible with <code>no aliens + normal people</code>, since if there were no aliens, then normal people wouldn’t hallucinate flying tic-tacs. The evidence is <em>also</em> incompatible with <code>normal aliens</code> since is those kinds of aliens were around they would make their existence obvious. However, the evidence fits pretty well with <code>weird aliens</code> and also with <code>no aliens + weird people</code>.</p>

<p>So, a reasonable model would be</p>

<div><div><pre><code>P[data | normal aliens] ≈ 0	
P[data | no aliens + normal people] ≈ 0
P[data | weird aliens] ≈ P[data | no aliens + weird people].
</code></pre></div></div>

<p><img src="/img/prior/cartoon6.svg" alt="" /></p>

<p>If we combine those assumptions, now we only get a 10% posterior probability of aliens.</p>

<div><div><pre><code>P[no aliens + normal people | data] ≈ 0
P[no aliens + weird people | data] ≈ 90%
P[normal aliens | data] ≈ 0
P[weird aliens | data] ≈ 10%
</code></pre></div></div>

<p><img src="/img/prior/cartoon7.svg" alt="" /></p>

<p>Now the results seem non-insane.</p>


  
(math)


  <p>To see why, first note that</p>

  <div><div><pre><code>P[normal aliens | data]
 ≈ P[data | no aliens + normal people]
 ≈ 0,
</code></pre></div>  </div>

  <p>since both <code>normal aliens</code> and <code>no aliens + normal people</code> have near-zero probability of producing the observed data.</p>

  <p>Meanwhile,</p>

  <div><div><pre><code>P[no aliens + weird people | data] / P[weird aliens | data]
 = P[no aliens + weird people, data] / P[weird aliens, data]
 ≈ P[no aliens + weird people] / P[weird aliens]
 ≈ .09 / .01
 = 9,
</code></pre></div>  </div>

  <p>where the second equality follows from the fact that the data is assumed to be equally likely under <code>no aliens + weird people</code> and <code>weird people</code></p>

  <p>It follows that</p>

  <div><div><pre><code>P[no aliens + normal people | data]
 ≈ 9 × P[weird aliens | data],
</code></pre></div>  </div>

  <p>and so</p>

  <div><div><pre><code>P[no aliens + weird people | data] ≈ 90%
P[weird aliens | data] ≈ 10%.
</code></pre></div>  </div>



<h2>Huh?</h2>

<p>I hope you are now confused. If not, let me lay out what’s strange: The priors for the two above models <em>both</em> say that there’s a 50% chance of aliens. The first prior wasn’t <em>wrong</em>, it was just less detailed than the second one.</p>

<p>That’s weird, because the second prior seemed to lead to completely different predictions. If a prior is non-wrong and the math is non-wrong, shouldn’t your answers be non-wrong? What the hell?</p>

<p>The simple explanation is that I’ve been lying to you a little bit. Take any situation where you’re trying to determine the truth of anything. Then there’s some space of <strong>things that could be true</strong>.</p>

<p><img src="/img/prior/cartoon1.svg" alt="" /></p>

<p>In some cases, this space is finite. If you’ve got a single tritium atom and you wait a year, either the atom decays or it doesn’t. But in most cases, there’s a large or infinite space of possibilities. Instead of you just being “sick” or “not sick”, you could be “high temperature but in good spirits” or “seems fine except won’t stop eating onions”.</p>

<p>(Usually the space of things that could be true isn’t easy to map to a small 1-D interval. I’m drawing like that for the sake of visualization, but really you should think of it as some high-dimensional space, or even an infinite dimensional space.)</p>

<p>In the case of aliens, the space of things that could be true might include, “There are lots of slimy aliens and a small number of furry aliens and the slimy aliens are really shy and the furry aliens are afraid of squirrels.” So, in <em>principle</em>, what you should do is divide up the space of things that might be true into tons of extremely detailed things and give a probability to each.</p>

<p><img src="/img/prior/cartoon12.svg" alt="" /></p>

<p>Often, the space of things that could be true is infinite. So theoretically, if you really want to do things by the book, what you should <em>really</em> do is specify how plausible each of those (infinite) possibilities is.</p>

<p>After you’ve done that, you can look at the data. For each thing that could be true, you need to think about the probability of the data. Since there’s an infinite number of things that could be true, that’s an infinite number of probabilities you need to specify. You could picture it as some curve like this:</p>

<p><img src="/img/prior/cartoon8.svg" alt="" /></p>

<p>(That’s a generic curve, not one for aliens.)</p>

<p>To me, this is the most underrated problem with applying Bayesian reasoning to complex real-world situations: In practice, there are an infinite number of things that can be true. It’s a lot of work to specify prior probabilities for an infinite number of things. And it’s <em>also</em> a lot of work to specify the likelihood of your data given an infinite number of things.</p>

<p>So what do we do in practice? We simplify, usually by limiting creating grouping the space of things that could be true into some small number of discrete categories. For the above curve, you might break things down into these four equally-plausible possibilities.</p>

<p><img src="/img/prior/cartoon9.svg" alt="" /></p>

<p>Then you might estimate these data probabilities for each of those possibilities.</p>

<p><img src="/img/prior/cartoon10.svg" alt="" /></p>

<p>Then you could put those together to get this posterior:</p>

<p><img src="/img/prior/cartoon11.svg" alt="" /></p>

<p>That’s not bad. <em>But it is just an approximation</em>. Your “real” posterior probabilities correspond to these areas:</p>

<p><img src="/img/prior/cartoon13.svg" alt="" /></p>

<p>That approximation was pretty good. But the <em>reason</em> it was good is that we started out with a good discretization of the space of things that might be true: One where the likelihood of the data didn’t vary too much for the different possibilities inside of <code>A</code>, <code>B</code>, <code>C</code>, and <code>D</code>. Imagine the likelihood of the data—if you were able to think about all the infinite possibilities one by one—looked like this:</p>

<p><img src="/img/prior/cartoon14.svg" alt="" /></p>

<p>This is dangerous. The problem is that you can’t actually think about all those infinite possibilities. When you think about four four discrete possibilities, you might estimate some likelihood that looks like this:</p>

<p><img src="/img/prior/cartoon15.svg" alt="" /></p>

<p>If you did that, that would lead to you underestimating the probability of <code>A</code>, <code>B</code>, and <code>C</code>, and overestimating the probability of <code>D</code>.</p>

<p>This is where my first model of aliens went wrong. My prior <code>P[aliens]</code> was not wrong. (Not to me.) The mistake was in assigning the same value to <code>P[data | aliens]</code> and <code>P[data | no aliens]</code>. Sure, I think the probability of all our alien-esque data is equally likely given aliens and given no-aliens. But that’s only true for <em>certain kinds</em> of aliens, and <em>certain kinds</em> of no-aliens. And my prior for <em>those kinds</em> of aliens is much lower than for those kinds of non-aliens.</p>

<p>Technically, the fix to the first model is simple: Make <code>P[data | aliens]</code> lower. But the <em>reason</em> it’s lower is that I have additional prior information that I forgot to include in my original prior. If I just assert that <code>P[data | aliens]</code> is much lower than <code>P[data | no aliens]</code> then the whole formal Bayesian thing isn’t actually doing very much—I might as well just state that I think <code>P[aliens | data]</code> is low. If I want to formally justify why <code>P[data | aliens]</code> should be lower, that requires a messy recursive procedure where I sort of add that missing prior information and then integrate it out when computing the data likelihood.</p>


  
(math)


  <p>Mathematically,</p>

  <div><div><pre><code>P[data | aliens]
 = ∫ P[wierd aliens | aliens]
 × P[data | wierd aliens] d(weird aliens)
 + ∫ P[normal aliens | aliens]
 × P[data | normal aliens] d(normal aliens).
</code></pre></div>  </div>

  <p>But now I have to give a detailed prior anyway. So what was the point of starting with a simple one?</p>



<p>I don’t think that technical fix is very good. While it’s technically correct (har-har) it’s very unintuitive. The better solution is what I did in the second model: To create a finer categorization of the space of things that might be true, such that the probability of the data is constant-ish for each term.</p>

<p>The thing is: Such a categorization depends on the data. Without seeing the actual data in our world, I would never have predicted that we would have so many pilots that report seeing tic-tacs. So I would never have predicted that I should have categories that are based on how much people might hallucinate evidence or how much aliens like to mess with us. So the only practical way to get good results is to <em>first</em> look at the data to figure out what categories are important, and <em>then</em> to ask yourself how likely you <em>would</em> have said those categories were, if you hadn’t yet seen any of the evidence.</p></div> <footer class="article-footer" data-astro-cid-wrgicudb> <div class="action-row" data-astro-cid-wrgicudb> <a href="https://dynomight.net/prior/" target="_blank" rel="noopener noreferrer" class="read-original-btn" data-astro-cid-wrgicudb>
阅读原文 ↗
</a> </div> <div class="back-to-list" data-astro-cid-wrgicudb> <a href="/reading-list" class="back-link" data-astro-cid-wrgicudb>← 返回订阅列表</a> </div> </footer> </article> </div>  </main> <footer class="site-footer" aria-label="Site footer" data-astro-cid-sz7xmlte> <div class="footer-content" data-astro-cid-sz7xmlte> <p class="footer-text" data-astro-cid-sz7xmlte>
&copy; 2026 AstroBlog
</p> <p class="footer-text" data-astro-cid-sz7xmlte>
Built with  <a href="https://astro.build/" target="_blank" rel="noopener noreferrer" class="footer-link" data-astro-cid-sz7xmlte>
Astro
</a> </p> </div> </footer>  </body></html> 