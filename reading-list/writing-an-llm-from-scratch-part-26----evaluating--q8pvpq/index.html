<!DOCTYPE html><html lang="zh-CN" data-astro-cid-sckkx6r4> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v5.17.1"><meta name="description" content="This post is on the second half of chapter 7 of
Sebastian Raschka's book
&#34;Build a Large Language Model (from Scratch)&#34;.
In the last post I covered
the part of the chapter that covers instruction fine-"><meta http-equiv="Content-Security-Policy" content="default-src 'self'; style-src 'self' 'unsafe-inline'; script-src 'self' 'unsafe-inline'; img-src 'self' data: https:; connect-src 'self'; font-src 'self' data:;"><link rel="canonical" href="https://blog.yuyins.com/reading-list/writing-an-llm-from-scratch-part-26----evaluating--q8pvpq/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://blog.yuyins.com/reading-list/writing-an-llm-from-scratch-part-26----evaluating--q8pvpq/"><meta property="og:title" content="Writing an LLM from scratch, part 26 -- evaluating the fine-tuned model"><meta property="og:description" content="This post is on the second half of chapter 7 of
Sebastian Raschka's book
&#34;Build a Large Language Model (from Scratch)&#34;.
In the last post I covered
the part of the chapter that covers instruction fine-"><meta property="og:image" content="https://blog.yuyins.com/favicon.svg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://blog.yuyins.com/reading-list/writing-an-llm-from-scratch-part-26----evaluating--q8pvpq/"><meta property="twitter:title" content="Writing an LLM from scratch, part 26 -- evaluating the fine-tuned model"><meta property="twitter:description" content="This post is on the second half of chapter 7 of
Sebastian Raschka's book
&#34;Build a Large Language Model (from Scratch)&#34;.
In the last post I covered
the part of the chapter that covers instruction fine-"><meta property="twitter:image" content="https://blog.yuyins.com/favicon.svg"><title>Writing an LLM from scratch, part 26 -- evaluating the fine-tuned model</title><!-- Theme and color scheme script (runs before page loads to prevent flash) --><script>
			// 初始化配色方案 - 默认使用 stone-amber
			const colorScheme = localStorage.getItem('colorScheme') || 'stone-amber';
			document.documentElement.setAttribute('data-color', colorScheme);

			// 强制使用浅色主题
			document.documentElement.setAttribute('data-theme', 'light');
		</script><link rel="stylesheet" href="/_astro/_slug_.ChsEGtcr.css">
<style>.blog-post-container[data-astro-cid-wrgicudb]{width:100%;margin:0 auto;padding:2rem 0}.blog-post[data-astro-cid-wrgicudb]{width:100%;min-width:0;max-width:42rem;margin:0 auto}.article-header[data-astro-cid-wrgicudb]{margin-bottom:2rem;padding-bottom:1rem;border-bottom:1px solid var(--border)}.article-header[data-astro-cid-wrgicudb] h1[data-astro-cid-wrgicudb]{font-size:2rem;line-height:1.3;margin-bottom:1rem;font-weight:700;color:var(--foreground);word-break:break-word}.article-meta[data-astro-cid-wrgicudb]{display:flex;align-items:center;gap:.5rem;font-size:.875rem;color:var(--muted-foreground)}.source-tag[data-astro-cid-wrgicudb]{background-color:var(--muted);color:var(--foreground);padding:.125rem .5rem;border-radius:9999px;font-weight:500;font-size:.75rem;text-decoration:none;transition:opacity .2s}.source-tag[data-astro-cid-wrgicudb]:hover{opacity:.8}.separator[data-astro-cid-wrgicudb]{color:var(--border)}.article-content[data-astro-cid-wrgicudb]{line-height:1.8;margin-bottom:3rem;font-size:1.0625rem;color:var(--foreground)}.article-content[data-astro-cid-wrgicudb] h2{margin-top:2rem;margin-bottom:1rem;font-size:1.5rem;font-weight:600}.article-content[data-astro-cid-wrgicudb] h3{margin-top:1.5rem;margin-bottom:.75rem;font-size:1.25rem;font-weight:600}.article-content[data-astro-cid-wrgicudb] p{margin-bottom:1.25rem}.article-content[data-astro-cid-wrgicudb] a{color:var(--accent);text-decoration:underline;text-underline-offset:2px}.article-content[data-astro-cid-wrgicudb] img{max-width:100%;height:auto;border-radius:.5rem;margin:1.5rem 0}.article-content[data-astro-cid-wrgicudb] blockquote{border-left:4px solid var(--border);padding-left:1rem;margin:1.5rem 0;color:var(--muted-foreground);font-style:italic}.article-content[data-astro-cid-wrgicudb] pre{background:var(--muted);padding:1rem;border-radius:.5rem;overflow-x:auto;font-family:monospace;margin:1.5rem 0}.article-content[data-astro-cid-wrgicudb] ul,.article-content[data-astro-cid-wrgicudb] ol{padding-left:1.5rem;margin-bottom:1.25rem}.article-content[data-astro-cid-wrgicudb] li{margin-bottom:.5rem}.article-footer[data-astro-cid-wrgicudb]{padding-top:2rem;border-top:1px solid var(--border);display:flex;flex-direction:column;gap:1.5rem;align-items:center}.action-row[data-astro-cid-wrgicudb]{display:flex;justify-content:center}.read-original-btn[data-astro-cid-wrgicudb]{display:inline-flex;align-items:center;gap:.5rem;padding:.75rem 1.5rem;background-color:var(--foreground);color:var(--background);border-radius:.5rem;font-weight:500;text-decoration:none;transition:opacity .2s}.read-original-btn[data-astro-cid-wrgicudb]:hover{opacity:.9}.back-to-list[data-astro-cid-wrgicudb]{text-align:center}.back-link[data-astro-cid-wrgicudb]{color:var(--muted-foreground);text-decoration:none;font-size:.875rem;transition:color .2s}.back-link[data-astro-cid-wrgicudb]:hover{color:var(--foreground)}
</style></head> <body data-astro-cid-sckkx6r4> <a id="skip-to-content" href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:top-4 focus:left-4 focus:z-50 focus:px-4 focus:py-2 focus:bg-accent focus:text-background" data-astro-cid-sckkx6r4>Skip to content</a> <header class="header" data-astro-cid-3ef6ksr2> <nav class="header-nav" aria-label="Main navigation" data-astro-cid-3ef6ksr2> <a href="/" class="header-logo" data-astro-cid-3ef6ksr2>
AstroBlog
</a> <div class="header-links" data-astro-cid-3ef6ksr2> <ul class="nav-list" data-astro-cid-3ef6ksr2> <li data-astro-cid-3ef6ksr2> <a href="/" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 文章 </a> </li><li data-astro-cid-3ef6ksr2> <a href="/category" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 分类 </a> </li><li data-astro-cid-3ef6ksr2> <a href="/reading-list" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 订阅 </a> </li><li data-astro-cid-3ef6ksr2> <a href="/about" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 关于 </a> </li> </ul> <div class="header-actions" data-astro-cid-3ef6ksr2> <div id="search" data-astro-cid-otpdt6jm> <button id="search-button" class="search-toggle" aria-label="打开搜索" title="搜索 (⌘K)" data-astro-cid-otpdt6jm> <svg class="search-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-astro-cid-otpdt6jm> <circle cx="11" cy="11" r="7" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-otpdt6jm></circle> <path d="M20 20L16.5 16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-otpdt6jm></path> </svg> </button> <dialog id="search-dialog" class="search-modal" data-astro-cid-otpdt6jm> <div class="modal-content" data-astro-cid-otpdt6jm> <div class="modal-header" data-astro-cid-otpdt6jm> <h2 class="modal-title" data-astro-cid-otpdt6jm>搜索文章</h2> <button id="close-search" class="close-button" aria-label="关闭搜索" data-astro-cid-otpdt6jm> <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-astro-cid-otpdt6jm> <path d="M18 6L6 18M6 6l12 12" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" data-astro-cid-otpdt6jm></path> </svg> </button> </div> <div id="pagefind-ui" data-astro-cid-otpdt6jm></div> </div> </dialog> </div> <script type="module">let d=!1;const c="pagefind-loaded";async function l(t=0){if(sessionStorage.getItem(c)==="true")return g(),!0;const r=document.getElementById("pagefind-ui");if(!r)return!1;r.innerHTML='<div style="padding: 2rem; text-align: center; color: var(--muted-foreground);">加载中...</div>';try{return await new Promise((o,s)=>{const a=document.createElement("script");a.src="/pagefind/pagefind-ui.js",a.onload=o,a.onerror=s,document.head.appendChild(a)}),g(),sessionStorage.setItem(c,"true"),!0}catch(i){return console.error("Failed to load search:",i),t<2?(console.log(`Retrying search load... (${t+1}/2)`),await new Promise(o=>setTimeout(o,1e3)),l(t+1)):(r&&(r.innerHTML=`
          <div style="padding: 2rem; text-align: center;">
            <p style="color: var(--muted-foreground); margin-bottom: 1rem;">搜索功能加载失败</p>
            <button id="retry-search" style="padding: 0.5rem 1rem; background: var(--accent); color: var(--accent-foreground); border: none; border-radius: 0.375rem; cursor: pointer;">
              重试
            </button>
          </div>
        `,document.getElementById("retry-search")?.addEventListener("click",()=>{sessionStorage.removeItem(c),l(0)})),!1)}}function g(){typeof PagefindUI<"u"&&(new PagefindUI({element:"#pagefind-ui",showSubResults:!0,showImages:!1,excerptLength:15,translations:{placeholder:"搜索文章...",clear_search:"清除",load_more:"加载更多",search_label:"搜索此站点",filters_label:"筛选",zero_results:"未找到结果 [SEARCH_TERM]",many_results:"找到 [COUNT] 个结果 [SEARCH_TERM]",one_result:"找到 [COUNT] 个结果 [SEARCH_TERM]",alt_search:"未找到 [SEARCH_TERM] 的结果。显示 [DIFFERENT_TERM] 的结果",search_suggestion:"未找到 [SEARCH_TERM] 的结果。尝试以下搜索：",searching:"搜索中 [SEARCH_TERM]..."}}),setTimeout(()=>{const t=document.querySelector(".pagefind-ui__search-input");t instanceof HTMLElement&&t.focus()},100))}function u(){if(d)return;d=!0;const t=document.getElementById("search-button"),n=document.getElementById("search-dialog"),r=document.getElementById("close-search");if(!t||!n||!r)return;let i=sessionStorage.getItem(c)==="true";const o=async()=>{n.showModal(),document.body.style.overflow="hidden",i?setTimeout(()=>{const e=document.querySelector(".pagefind-ui__search-input");e instanceof HTMLElement&&e.focus()},100):await l()&&(i=!0)},s=()=>{n.close(),document.body.style.overflow=""},a=e=>{e.target===n&&s()},m=e=>{e.key==="Escape"&&n.open&&s()},f=e=>{(e.metaKey||e.ctrlKey)&&e.key==="k"&&(e.preventDefault(),n.open?s():o())};t.addEventListener("click",o),r.addEventListener("click",s),n.addEventListener("click",a),document.addEventListener("keydown",m),document.addEventListener("keydown",f),document.addEventListener("astro:before-swap",()=>{t.removeEventListener("click",o),r.removeEventListener("click",s),n.removeEventListener("click",a),document.removeEventListener("keydown",m),document.removeEventListener("keydown",f),d=!1},{once:!0})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",u):u();document.addEventListener("astro:page-load",u);</script> <link href="/pagefind/pagefind-ui.css" rel="stylesheet">  </div> </div> </nav> </header>  <main id="main-content" data-pagefind-body class="main-content main-content--wide" data-astro-cid-sckkx6r4>  <div class="blog-post-container" data-astro-cid-wrgicudb> <article class="blog-post" data-astro-cid-wrgicudb> <header class="article-header" data-astro-cid-wrgicudb> <h1 data-astro-cid-wrgicudb>Writing an LLM from scratch, part 26 -- evaluating the fine-tuned model</h1> <div class="article-meta" data-astro-cid-wrgicudb> <a href="https://gilesthomas.com" target="_blank" rel="noopener noreferrer" class="source-tag" data-astro-cid-wrgicudb>gilesthomas.com</a> <span class="separator" data-astro-cid-wrgicudb>·</span> <time data-astro-cid-wrgicudb>2025年11月3日</time> </div> </header> <!-- 使用清洗后的安全 HTML 内容 --> <div class="article-content app-prose" data-astro-cid-wrgicudb><p>This post is on the second half of chapter 7 of
<a href="https://sebastianraschka.com/" target="_blank" rel="noopener noreferrer">Sebastian Raschka</a>'s book
"<a href="https://www.manning.com/books/build-a-large-language-model-from-scratch" target="_blank" rel="noopener noreferrer">Build a Large Language Model (from Scratch)</a>".
In the <a href="/2025/10/llm-from-scratch-25-instruction-fine-tuning" target="_blank" rel="noopener noreferrer">last post</a> I covered
the part of the chapter that covers instruction fine-tuning; this time round,
we evaluate our model -- particularly interestingly, we try using another, smarter,
model to judge how good its responses are.</p>

<p>Once again, Raschka's explanation in this section is very clear, and there's not
that much that was conceptually new to me, so I don't have that many
notes -- in fact, this post is probably the shortest one in my series so far!</p>
<h3>Generating the test set responses</h3>

<p>Unusually, when at the start of section 7.7 we generate
some sample responses for the instructions in our test set, I got exactly the same
results as in the book.  For once, I guess, everything that uses randomness was
happening in the same order as it did when Raschka ran it on his machine.</p>

<p>The next step was to generate a file with all of the responses to all of the
test instructions, which took 18.9 seconds on my RTX 3090 (compared to
a minute on an A100, per the book -- that's quite surprising!)</p>

<p>Once that was done, it was time to install Ollama so that I could use the Llama 3
model to evaluate my own.</p>

<h3>Ollama</h3>

<p>I've never used Ollama before -- when playing with other people's models, I've always
used <a href="https://huggingface.co/docs/transformers/en/index" target="_blank" rel="noopener noreferrer">Hugging Face's Transformers</a>
library.</p>

<p>It's a neat package, though.  It wraps <code>llama.cpp</code>, which is a pure C/C++ inference
framework (with CUDA support), and makes it easy to download and run models that
have been packaged for it.  Being written in C, I would imagine that it's faster than
PyTorch/Transformers -- though, being inference-only, it's less useful if you're planning to do things
like training or fine-tuning the models.</p>

<p>My desktop is running a fairly customised install of Arch Linux, and I didn't want to
use the default install procedure (which puts it into your system-wide <code>/bin</code> and <code>/lib</code>
directories).  But it turns out that it's a very well-packaged app,
and you don't need to do that.</p>

<p>Using the manual <a href="https://docs.ollama.com/linux" target="_blank" rel="noopener noreferrer">install instructions for Linux</a>,
I just created a new directory <code>~/Dev/ollama</code>, and then <code>cd</code>ed there and downloaded it:</p>

<div>
<pre><span></span><code>wget<span> </span>https://ollama.com/download/ollama-linux-amd64.tgz
</code></pre>
</div>

<p>It was about 1.75 GiB.  I then untarred it:</p>

<div>
<pre><span></span><code>tar<span> </span>xf<span> </span>ollama-linux-amd64.tgz
</code></pre>
</div>

<p>...and then I could run commands with full paths, for example:</p>

<div>
<pre><span></span><code>~/Dev/ollama/bin/ollama<span> </span>serve
</code></pre>
</div>

<p>...to start up the server, or</p>

<div>
<pre><span></span><code>~/Dev/ollama/bin/ollama<span> </span>run<span> </span>llama3
</code></pre>
</div>

<p>...to start a session.</p>

<p>Neat!  It's always good to see pre-built binary packages that have no issues with their install location.</p>

<h3>Actually running the evaluation</h3>

<p>The next step was to throw all of the generated test responses (and their associated
targets) at Llama 3 and see what it thought about how close they were.</p>

<p>Again, this all worked without trouble.  I noted that the responses I was getting from Llama 3
were not the same as the ones in the book -- Raschka notes that
Ollama is non-deterministic, so there's no surprise there (though it does make me
wonder why it accepts a <code>seed</code> parameter in the API call).</p>

<p>When I got on to the final eval, where you run the test results through Llama 3
and ask it to rate them compared to the target outputs, it took 11 seconds to run,
and I got an average score of 48.95 / 100, which is close enough to the 50.32 that
appears in the book. <sup><a href="#fn-1" target="_blank" rel="noopener noreferrer">1</a></sup> I'd run an eval on my model, using a smarter model to judge
its responses!</p>

<p>Somewhat surprisingly, that number was stable over multiple runs.  So perhaps there
is some level of determinism in Ollama now that wasn't present when the book was written,
and the seed (eg. <code>123</code>) is of value.  Or perhaps Raschka's comment about it being non-deterministic
was more of a "between machines" thing rather than for multiple runs on the same machine
-- though then I'm not sure why he suggests re-running it for multiple results.</p>

<p>Anyway -- that was it!  Eval done.  And, to my amazement, that was the end of
the chapter -- and almost the end of the  book.  We've built an LLM from scratch,
fine-tuned it, and evaluated it by using a smarter model to judge how well it was
following instructions.</p>

<h3>This is the end...</h3>

<p>...or at least the end of the beginning.</p>

<p>Having run the evaluation, I've reached the end of the main part of
"<a href="https://www.manning.com/books/build-a-large-language-model-from-scratch" target="_blank" rel="noopener noreferrer">Build a Large Language Model (from Scratch)</a>".
But I don't think I've reached the end of this project, there's still more to do
(not least working through the appendices).</p>

<p>So, coming up next: a post summarising what I've got through so far in this series,
and what the next steps are to wrap it up.</p>

<p><a href="/2025/11/llm-from-scratch-27-whats-left-and-whats-next" target="_blank" rel="noopener noreferrer">Here's a link to the next post in this series</a>.</p>

<div>
<hr />
<ol>
<li>
<p>I also got 110 out of 110 scores -- that is, every response from Llama 3
was parseable as an integer.  That actually kind of surprised me!  Models like
to be chatty and helpful.  But looking into it,
<a href="https://x.com/goodside/status/1657396491676164096" target="_blank" rel="noopener noreferrer">the famous X post by Riley Goodside</a>
where he had to "threaten" Bard to stop it from saying "Sure, no problem!  Here's your
JSON" was almost two years ago. <a href="#fnref-1" target="_blank" rel="noopener noreferrer">↩</a></p>
</li>
</ol>
</div>
</div> <footer class="article-footer" data-astro-cid-wrgicudb> <div class="action-row" data-astro-cid-wrgicudb> <a href="https://www.gilesthomas.com/2025/11/llm-from-scratch-26-evaluating-the-fine-tuned-model" target="_blank" rel="noopener noreferrer" class="read-original-btn" data-astro-cid-wrgicudb>
阅读原文 ↗
</a> </div> <div class="back-to-list" data-astro-cid-wrgicudb> <a href="/reading-list" class="back-link" data-astro-cid-wrgicudb>← 返回订阅列表</a> </div> </footer> </article> </div>  </main> <footer class="site-footer" aria-label="Site footer" data-astro-cid-sz7xmlte> <div class="footer-content" data-astro-cid-sz7xmlte> <p class="footer-text" data-astro-cid-sz7xmlte>
&copy; 2026 AstroBlog
</p> <p class="footer-text" data-astro-cid-sz7xmlte>
Built with  <a href="https://astro.build/" target="_blank" rel="noopener noreferrer" class="footer-link" data-astro-cid-sz7xmlte>
Astro
</a> </p> </div> </footer>  </body></html> 