<!DOCTYPE html><html lang="zh-CN" data-astro-cid-sckkx6r4> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v5.16.1"><meta name="description" content="Chapter 7 of Sebastian Raschka's book
&#34;Build a Large Language Model (from Scratch)&#34;
explains how we fine-tune our LLM to follow instructions -- essentially turning
a model that can do next-token compl"><link rel="canonical" href="https://blog.yuyins.com/reading-list/writing-an-llm-from-scratch-part-24----the-transcr-g32dpd/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://blog.yuyins.com/reading-list/writing-an-llm-from-scratch-part-24----the-transcr-g32dpd/"><meta property="og:title" content="Writing an LLM from scratch, part 24 -- the transcript hack"><meta property="og:description" content="Chapter 7 of Sebastian Raschka's book
&#34;Build a Large Language Model (from Scratch)&#34;
explains how we fine-tune our LLM to follow instructions -- essentially turning
a model that can do next-token compl"><meta property="og:image" content="https://blog.yuyins.com/favicon.svg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://blog.yuyins.com/reading-list/writing-an-llm-from-scratch-part-24----the-transcr-g32dpd/"><meta property="twitter:title" content="Writing an LLM from scratch, part 24 -- the transcript hack"><meta property="twitter:description" content="Chapter 7 of Sebastian Raschka's book
&#34;Build a Large Language Model (from Scratch)&#34;
explains how we fine-tune our LLM to follow instructions -- essentially turning
a model that can do next-token compl"><meta property="twitter:image" content="https://blog.yuyins.com/favicon.svg"><title>Writing an LLM from scratch, part 24 -- the transcript hack</title><!-- Theme and color scheme script (runs before page loads to prevent flash) --><script>
			// 初始化配色方案 - 默认使用 stone-amber
			const colorScheme = localStorage.getItem('colorScheme') || 'stone-amber';
			document.documentElement.setAttribute('data-color', colorScheme);

			// 强制使用浅色主题
			document.documentElement.setAttribute('data-theme', 'light');
		</script><link rel="stylesheet" href="/_astro/_slug_.C-G01a-E.css">
<style>.blog-post-container[data-astro-cid-wrgicudb]{width:100%;margin:0 auto;padding:2rem 0}.blog-post[data-astro-cid-wrgicudb]{width:100%;min-width:0;max-width:42rem;margin:0 auto}.article-header[data-astro-cid-wrgicudb]{margin-bottom:2rem;padding-bottom:1rem;border-bottom:1px solid var(--border)}.article-header[data-astro-cid-wrgicudb] h1[data-astro-cid-wrgicudb]{font-size:2rem;line-height:1.3;margin-bottom:1rem;font-weight:700;color:var(--foreground);word-break:break-word}.article-meta[data-astro-cid-wrgicudb]{display:flex;align-items:center;gap:.5rem;font-size:.875rem;color:var(--muted-foreground)}.source-tag[data-astro-cid-wrgicudb]{background-color:var(--muted);color:var(--foreground);padding:.125rem .5rem;border-radius:9999px;font-weight:500;font-size:.75rem;text-decoration:none;transition:opacity .2s}.source-tag[data-astro-cid-wrgicudb]:hover{opacity:.8}.separator[data-astro-cid-wrgicudb]{color:var(--border)}.article-content[data-astro-cid-wrgicudb]{line-height:1.8;margin-bottom:3rem;font-size:1.0625rem;color:var(--foreground)}.article-content[data-astro-cid-wrgicudb] h2{margin-top:2rem;margin-bottom:1rem;font-size:1.5rem;font-weight:600}.article-content[data-astro-cid-wrgicudb] h3{margin-top:1.5rem;margin-bottom:.75rem;font-size:1.25rem;font-weight:600}.article-content[data-astro-cid-wrgicudb] p{margin-bottom:1.25rem}.article-content[data-astro-cid-wrgicudb] a{color:var(--accent);text-decoration:underline;text-underline-offset:2px}.article-content[data-astro-cid-wrgicudb] img{max-width:100%;height:auto;border-radius:.5rem;margin:1.5rem 0}.article-content[data-astro-cid-wrgicudb] blockquote{border-left:4px solid var(--border);padding-left:1rem;margin:1.5rem 0;color:var(--muted-foreground);font-style:italic}.article-content[data-astro-cid-wrgicudb] pre{background:var(--muted);padding:1rem;border-radius:.5rem;overflow-x:auto;font-family:monospace;margin:1.5rem 0}.article-content[data-astro-cid-wrgicudb] ul,.article-content[data-astro-cid-wrgicudb] ol{padding-left:1.5rem;margin-bottom:1.25rem}.article-content[data-astro-cid-wrgicudb] li{margin-bottom:.5rem}.article-footer[data-astro-cid-wrgicudb]{padding-top:2rem;border-top:1px solid var(--border);display:flex;flex-direction:column;gap:1.5rem;align-items:center}.action-row[data-astro-cid-wrgicudb]{display:flex;justify-content:center}.read-original-btn[data-astro-cid-wrgicudb]{display:inline-flex;align-items:center;gap:.5rem;padding:.75rem 1.5rem;background-color:var(--foreground);color:var(--background);border-radius:.5rem;font-weight:500;text-decoration:none;transition:opacity .2s}.read-original-btn[data-astro-cid-wrgicudb]:hover{opacity:.9}.back-to-list[data-astro-cid-wrgicudb]{text-align:center}.back-link[data-astro-cid-wrgicudb]{color:var(--muted-foreground);text-decoration:none;font-size:.875rem;transition:color .2s}.back-link[data-astro-cid-wrgicudb]:hover{color:var(--foreground)}
</style></head> <body data-astro-cid-sckkx6r4> <a id="skip-to-content" href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:top-4 focus:left-4 focus:z-50 focus:px-4 focus:py-2 focus:bg-accent focus:text-background" data-astro-cid-sckkx6r4>Skip to content</a> <header class="header" data-astro-cid-3ef6ksr2> <nav class="header-nav" aria-label="Main navigation" data-astro-cid-3ef6ksr2> <a href="/" class="header-logo" data-astro-cid-3ef6ksr2>
AstroBlog
</a> <div class="header-links" data-astro-cid-3ef6ksr2> <ul class="nav-list" data-astro-cid-3ef6ksr2> <li data-astro-cid-3ef6ksr2> <a href="/" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 文章 </a> </li><li data-astro-cid-3ef6ksr2> <a href="/category" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 分类 </a> </li><li data-astro-cid-3ef6ksr2> <a href="/reading-list" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 订阅 </a> </li><li data-astro-cid-3ef6ksr2> <a href="/about" class="nav-link" data-astro-prefetch="true" data-astro-cid-3ef6ksr2> 关于 </a> </li> </ul> <div class="header-actions" data-astro-cid-3ef6ksr2> <div id="search" data-astro-cid-otpdt6jm> <button id="search-button" class="search-toggle" aria-label="打开搜索" title="搜索 (⌘K)" data-astro-cid-otpdt6jm> <svg class="search-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-astro-cid-otpdt6jm> <circle cx="11" cy="11" r="7" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-otpdt6jm></circle> <path d="M20 20L16.5 16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-otpdt6jm></path> </svg> </button> <dialog id="search-dialog" class="search-modal" data-astro-cid-otpdt6jm> <div class="modal-content" data-astro-cid-otpdt6jm> <div class="modal-header" data-astro-cid-otpdt6jm> <h2 class="modal-title" data-astro-cid-otpdt6jm>搜索文章</h2> <button id="close-search" class="close-button" aria-label="关闭搜索" data-astro-cid-otpdt6jm> <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" data-astro-cid-otpdt6jm> <path d="M18 6L6 18M6 6l12 12" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" data-astro-cid-otpdt6jm></path> </svg> </button> </div> <div id="pagefind-ui" data-astro-cid-otpdt6jm></div> </div> </dialog> </div> <script type="module">let d=!1;const c="pagefind-loaded";async function l(t=0){if(sessionStorage.getItem(c)==="true")return g(),!0;const r=document.getElementById("pagefind-ui");if(!r)return!1;r.innerHTML='<div style="padding: 2rem; text-align: center; color: var(--muted-foreground);">加载中...</div>';try{return await new Promise((o,s)=>{const a=document.createElement("script");a.src="/pagefind/pagefind-ui.js",a.onload=o,a.onerror=s,document.head.appendChild(a)}),g(),sessionStorage.setItem(c,"true"),!0}catch(i){return console.error("Failed to load search:",i),t<2?(console.log(`Retrying search load... (${t+1}/2)`),await new Promise(o=>setTimeout(o,1e3)),l(t+1)):(r&&(r.innerHTML=`
          <div style="padding: 2rem; text-align: center;">
            <p style="color: var(--muted-foreground); margin-bottom: 1rem;">搜索功能加载失败</p>
            <button id="retry-search" style="padding: 0.5rem 1rem; background: var(--accent); color: var(--accent-foreground); border: none; border-radius: 0.375rem; cursor: pointer;">
              重试
            </button>
          </div>
        `,document.getElementById("retry-search")?.addEventListener("click",()=>{sessionStorage.removeItem(c),l(0)})),!1)}}function g(){typeof PagefindUI<"u"&&(new PagefindUI({element:"#pagefind-ui",showSubResults:!0,showImages:!1,excerptLength:15,translations:{placeholder:"搜索文章...",clear_search:"清除",load_more:"加载更多",search_label:"搜索此站点",filters_label:"筛选",zero_results:"未找到结果 [SEARCH_TERM]",many_results:"找到 [COUNT] 个结果 [SEARCH_TERM]",one_result:"找到 [COUNT] 个结果 [SEARCH_TERM]",alt_search:"未找到 [SEARCH_TERM] 的结果。显示 [DIFFERENT_TERM] 的结果",search_suggestion:"未找到 [SEARCH_TERM] 的结果。尝试以下搜索：",searching:"搜索中 [SEARCH_TERM]..."}}),setTimeout(()=>{const t=document.querySelector(".pagefind-ui__search-input");t instanceof HTMLElement&&t.focus()},100))}function u(){if(d)return;d=!0;const t=document.getElementById("search-button"),n=document.getElementById("search-dialog"),r=document.getElementById("close-search");if(!t||!n||!r)return;let i=sessionStorage.getItem(c)==="true";const o=async()=>{n.showModal(),document.body.style.overflow="hidden",i?setTimeout(()=>{const e=document.querySelector(".pagefind-ui__search-input");e instanceof HTMLElement&&e.focus()},100):await l()&&(i=!0)},s=()=>{n.close(),document.body.style.overflow=""},a=e=>{e.target===n&&s()},m=e=>{e.key==="Escape"&&n.open&&s()},f=e=>{(e.metaKey||e.ctrlKey)&&e.key==="k"&&(e.preventDefault(),n.open?s():o())};t.addEventListener("click",o),r.addEventListener("click",s),n.addEventListener("click",a),document.addEventListener("keydown",m),document.addEventListener("keydown",f),document.addEventListener("astro:before-swap",()=>{t.removeEventListener("click",o),r.removeEventListener("click",s),n.removeEventListener("click",a),document.removeEventListener("keydown",m),document.removeEventListener("keydown",f),d=!1},{once:!0})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",u):u();document.addEventListener("astro:page-load",u);</script> <link href="/pagefind/pagefind-ui.css" rel="stylesheet">  </div> </div> </nav> </header>  <main id="main-content" data-pagefind-body class="main-content main-content--wide" data-astro-cid-sckkx6r4>  <div class="blog-post-container" data-astro-cid-wrgicudb> <article class="blog-post" data-astro-cid-wrgicudb> <header class="article-header" data-astro-cid-wrgicudb> <h1 data-astro-cid-wrgicudb>Writing an LLM from scratch, part 24 -- the transcript hack</h1> <div class="article-meta" data-astro-cid-wrgicudb> <a href="https://gilesthomas.com" target="_blank" rel="noopener noreferrer" class="source-tag" data-astro-cid-wrgicudb>gilesthomas.com</a> <span class="separator" data-astro-cid-wrgicudb>·</span> <time data-astro-cid-wrgicudb>2025年10月28日</time> </div> </header> <!-- 使用清洗后的安全 HTML 内容 --> <div class="article-content app-prose" data-astro-cid-wrgicudb><p>Chapter 7 of <a href="https://sebastianraschka.com/" target="_blank" rel="noopener noreferrer">Sebastian Raschka</a>'s book
"<a href="https://www.manning.com/books/build-a-large-language-model-from-scratch" target="_blank" rel="noopener noreferrer">Build a Large Language Model (from Scratch)</a>"
explains how we fine-tune our LLM to follow instructions -- essentially turning
a model that can do next-token completion for text generation into something we
can use for a chatbot.</p>

<p>Back when I <a href="/2023/03/ai-llm-bot-beginners-tutorial-01" target="_blank" rel="noopener noreferrer">first started looking into LLMs</a>,
I used a setup that didn't require that, and got surprisingly good results, at least
with later OpenAI models.</p>

<p>The trick was to present the text as something that made sense
in the context of next-token prediction.  Instead of just throwing something like
this at the LLM:</p>

<pre><code>User: Provide a synonym for 'bright'

Bot:
</code></pre>

<p>...you would instead prepare it with an introductory paragraph, like this:</p>

<pre><code>This is a transcript of a conversation between a helpful bot, 'Bot', and a human,
'User'.  The bot is very intelligent and always answers the human's questions
with a useful reply.

User: Provide a synonym for 'bright'

Bot:
</code></pre>

<p>Earlier OpenAI models couldn't do this when I accessed them through the API, but
later ones could.</p>

<p>How does our GPT-2 model stack up with this kind of thing -- and for comparison,
how about a newer, more sophisticated base (as in, not instruction fine-tuned)
model?</p>
<h3>Comparing the GPT-2 models</h3>

<p>I wrote a <a href="https://github.com/gpjt/llm-from-scratch/blob/main/transcript-hack.py" target="_blank" rel="noopener noreferrer">simple script</a>
that just allowed me to test the transcript above against
different GPT-2 models, using the infrastructure that I'd already written for the
code in the book.  It uses the <code>generate_text_simple</code> function -- that is, the
basic one using greedy decoding (always pick the most likely next token) and just
gets the next 23 tokens.</p>

<p>Here's what I got with the different GPT-2 models:</p>

<h4>gpt2-small (124M)</h4>

<pre><code>This is a transcript of a conversation between a helpful bot, 'Bot', and a human,
'User'.  The bot is very intelligent and always answers the human's questions
with a useful reply.

User: Provide a synonym for 'bright'

Bot:

User:

Bot:

User:

Bot:

User
</code></pre>

<p>It looks like it has the concept of a transcript, at least -- but not very useful.</p>

<h4>gpt2-medium (355M)</h4>

<pre><code>This is a transcript of a conversation between a helpful bot, 'Bot', and a human,
'User'.  The bot is very intelligent and always answers the human's questions
with a useful reply.

User: Provide a synonym for 'bright'

Bot:  Bright is a synonym for bright.

User:  Bright is a synonym for bright.
</code></pre>

<p>Better in a way -- it's got some actual text in there.</p>

<h4>gpt2-large (774M)</h4>

<pre><code>This is a transcript of a conversation between a helpful bot, 'Bot', and a human,
'User'.  The bot is very intelligent and always answers the human's questions
with a useful reply.

User: Provide a synonym for 'bright'

Bot: ಠ_ಠ

User: Provide a synonym for 'bright'

Bot:
</code></pre>

<p>Hmm.  Still not looking good, and that dodgy Unicode isn't great (unless the bot
was trying to say <code>^_^</code> ;-)</p>

<h4>gpt2-xl (1558M)</h4>

<pre><code>This is a transcript of a conversation between a helpful bot, 'Bot', and a human,
'User'.  The bot is very intelligent and always answers the human's questions
with a useful reply.

User: Provide a synonym for 'bright'

Bot: ????

User: ????

Bot: ????

User: ????

Bot:
</code></pre>

<p>OK, still not getting there.</p>

<p>So it looks like the GPT-2 model can't do this.  That actually makes a lot of sense
-- at the time I wrote my post using this transcript hack, I found that the <code>ada</code>,
<code>babbage</code> and <code>curie</code> versions could not either, and nor could earlier versions of
<code>davinci</code>.  These were all versions of GPT-3, so one generation later than the
version we're working with here.  The first version that "got it" was
<code>text-davinci-003</code>, which was GPT-3.5.</p>

<p>But wait!</p>

<p>Looking back at my blog post way back when, I spotted something about the <code>text-davinci-003</code>
model.  At the time, the docs on the OpenAI site said that it can:</p>

<blockquote>
  <p>do any language task with better quality, longer output, and consistent
  instruction-following than the curie, babbage, or ada models</p>
</blockquote>

<p>Did it perhaps have instruction-following built in -- that is, had it been instruction
fine-tuned already?</p>

<p>That doesn't quite make sense, though, because instruction fine-tuning is normally
done using a particular format.  Imagine that a model was fine-tuned on chats that
looked like this <a href="https://huggingface.co/datasets/timdettmers/openassistant-guanaco" target="_blank" rel="noopener noreferrer">Guanaco</a>-format
dataset:</p>

<pre><code>### Human: Hello there
### Assistant: Hello, how are you?
### Human: I'm fine, how about you?
</code></pre>

<p>...and then it was fed something more like this (which is from a
<a href="https://www.reddit.com/r/LocalLLaMA/comments/155po2p/get_llama_2_prompt_format_right/" target="_blank" rel="noopener noreferrer">Reddit post about the Llama 2 prompt format</a>):</p>

<pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;
You are a helpful, respectful and honest assistant. Always answer as helpfully as
possible, while being safe.  Your answers should not include any harmful, unethical,
racist, sexist, toxic, dangerous, or illegal content. Please ensure that your
responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why
instead of answering something not correct. If you don't know the answer to a
question, please don't share false information.
&lt;&lt;/SYS&gt;&gt;

Hello there [/INST]
Hello, how are you?
[INST] I'm fine, how about you? [/INST]
</code></pre>

<p>That is, at least intuitively, going to cause problems.</p>

<p>What happens if we take a look at a more modern model, but make sure it's a base one?</p>

<h3>Qwen3-0.6B-Base</h3>

<p>I've been playing with <a href="https://huggingface.co/Qwen/Qwen3-0.6B-Base" target="_blank" rel="noopener noreferrer"><code>Qwen/Qwen3-0.6B-Base</code></a>,
so I wrote <a href="https://github.com/gpjt/llm-from-scratch/blob/main/qwen-transcript-hack.py" target="_blank" rel="noopener noreferrer">a script</a> to use it.
Here's what I got:</p>

<pre><code>This is a transcript of a conversation between a helpful bot, 'Bot', and a human,
'User'.  The bot is very intelligent and always answers the human's questions
with a useful reply.

User: Provide a synonym for 'bright'

Bot:  Brightness is a synonym for illumination, which is the process of shining
light into a space, such as a room or a room.

User: What's a synonym for 'bright'?

Bot: Brightness is a synonym for illumination, which is the process of shining
light into a space, such as a room or a room.

User: What's a synonym for 'bright'?

Bot:
</code></pre>

<p>That's actually not half-bad!  The first response is OK apart from the "a room or a room".  Its prediction of what
the user would say as their second question isn't great, though.   Let's see
what happens if we prompt it with a more reasonable second round -- I modified
the script to complete this sequence:</p>

<pre><code>This is a transcript of a conversation between a helpful bot, 'Bot', and a human,
'User'.  The bot is very intelligent and always answers the human's questions
with a useful reply.

User: Provide a synonym for 'bright'

Bot:  Brightness is a synonym for illumination, which is the process of shining
light into a space, such as a room or a room.

User: What is an antonym of 'complicated'?

Bot:
</code></pre>

<p>The full result (including the input sequence) was:</p>

<pre><code>This is a transcript of a conversation between a helpful bot, 'Bot', and a human,
'User'.  The bot is very intelligent and always answers the human's questions
with a useful reply.

User: Provide a synonym for 'bright'

Bot:  Brightness is a synonym for illumination, which is the process of shining
light into a space, such as a room or a room.

User: What is an antonym of 'complicated'?

Bot:  Complicated is an antonym for simple, which is easy and straightforward,
and easy to understand, which is easy and not difficult.

User: What is the difference between'reality' and 'fiction'?

Bot:  Reality is the real world or physical world, which is based on observable
facts and can be observed, and fiction is the story or idea created by a person
or a group
</code></pre>

<p>This is looking almost solid.
And there's no mention of instruction training on the
<a href="https://huggingface.co/Qwen/Qwen3-0.6B-Base" target="_blank" rel="noopener noreferrer">model card for <code>Qwen/Qwen3-0.6B-Base</code></a>.</p>

<h3>Conclusion</h3>

<p>I think that my initial intuition was right -- a sufficiently advanced base
model can operate as a chatbot without instruction fine-tuning.</p>

<p>However, I think I was actually wrong about it when I originally had the intuition!  The OpenAI
GPT-3.5 model that I believed was a base one, <code>text-davinci-003</code>, had already had some kind
of instruction-following tuning after it was initially trained.  It was really impressive
that it was able to generalise from that to the somewhat ad-hoc chat transcript format
that I was using at the time, but it was not a base model.</p>

<p>By contrast, a modern 600M-parameter model -- smaller than GPT-2 large (at 774M) and less than
half the size of GPT-2 XL (at 1.5B) -- can actually work with a transcript without
difficulty.</p>

<p>My guess is that as well as the architectural improvements that have happened over the
years since GPT-2, it's the size and quality of the dataset it was trained on.</p>

<p>GPT-2 was trained
on "8 million documents for a total of 40GB of text" according to
<a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener noreferrer">the paper</a>.
It's not entirely clear how many tokens that is, but I've seen claims of 10B tokens
(for example <a href="https://www.reddit.com/r/mlscaling/comments/1d3a793/andrej_karpathy_gpt2_124m_in_llmc_in_90_minutes/" target="_blank" rel="noopener noreferrer">here</a>),
and that seems in line with 40GB, as that would be 40 billion bytes, and 4 bytes/token
seems reasonable.</p>

<p>GPT-3, according to <a href="https://en.wikipedia.org/wiki/GPT-3" target="_blank" rel="noopener noreferrer">Wikipedia</a>, was trained on
around 500B tokens.</p>

<p>The Qwen3 series, by contrast, according to the <a href="https://huggingface.co/Qwen/Qwen3-0.6B-Base" target="_blank" rel="noopener noreferrer">model card</a>,
was trained on 36 trillion tokens across 119 languages.</p>

<p>That's 72 times as much data -- and the data was probably much more curated, too.
That's a big difference!  Perhaps it had seen lots of transcripts in there, and
that was why it was able to mimic them?  Or just lots of different kinds of text
in general?</p>

<p>I guess it's no surprise that training runs are getting ever-more expensive if that's the
size of a frontier model run, though.</p>

<p>So, that's all for the transcript trick -- base models actually can work as
chatbots without instruction fine-tuning, if they're sufficiently advanced
and trained on enough data.   That's useful to know!</p>

<p>Time to go back to the book; coming next, my notes on the actual fine-tuning
I was meant to be doing rather than messing around with this :-)</p>

<p><a href="/2025/10/llm-from-scratch-25-instruction-fine-tuning" target="_blank" rel="noopener noreferrer">Here's a link to the next post in this series</a>.</p>
</div> <footer class="article-footer" data-astro-cid-wrgicudb> <div class="action-row" data-astro-cid-wrgicudb> <a href="https://www.gilesthomas.com/2025/10/llm-from-scratch-24-the-transcript-hack" target="_blank" rel="noopener noreferrer" class="read-original-btn" data-astro-cid-wrgicudb>
阅读原文 ↗
</a> </div> <div class="back-to-list" data-astro-cid-wrgicudb> <a href="/reading-list" class="back-link" data-astro-cid-wrgicudb>← 返回订阅列表</a> </div> </footer> </article> </div>  </main> <footer class="site-footer" aria-label="Site footer" data-astro-cid-sz7xmlte> <div class="footer-content" data-astro-cid-sz7xmlte> <p class="footer-text" data-astro-cid-sz7xmlte>
&copy; 2026 AstroBlog
</p> <p class="footer-text" data-astro-cid-sz7xmlte>
Built with  <a href="https://astro.build/" target="_blank" rel="noopener noreferrer" class="footer-link" data-astro-cid-sz7xmlte>
Astro
</a> </p> </div> </footer>  </body></html> 